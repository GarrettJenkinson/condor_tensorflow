{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vcT_VpWpT1Kf"
   },
   "source": [
    "# CONDOR ordinal classification/regression in Tensorflow Keras \n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/GarrettJenkinson/condor_tensorflow/blob/main/docs/CONDOR_TensorFlow_demo.ipynb)\n",
    "\n",
    "\n",
    "This notebook uses MNIST hand-written digits and Amazon reviews as examples of ordinal classification, using the condor_tensorflow package for Tensorflow Keras.\n",
    "\n",
    "\n",
    "**Acknowledgments**: This notebook is based in part on PyTorch source code written by Sebastian Rashka [in this notebook](https://github.com/Raschka-research-group/coral-cnn/blob/master/coral-implementation-recipe.ipynb) and the CORAL ordinal notebook written by [Chris Kennedy and Stephen Matthews](https://github.com/ck37/coral-ordinal)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QsCIIgoFOkr"
   },
   "source": [
    "## Installation for Google Colab\n",
    "\n",
    "With pip you can either install the latest source code from GitHub or the stable version of the module on pypi.org"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#upgrade sklearn...only needed for advanced ordinalEncoder behaviours\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install scikit-learn==0.24.2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pO9cwoJ33G2q",
    "outputId": "9307fd1e-d4ce-4c23-f5f8-427f8713b579"
   },
   "outputs": [],
   "source": [
    "if 'google.colab' in str(get_ipython()):\n",
    "    GITHUB_AUTH = \"GarrettJenkinson:<APIaccessTOKEN>\"\n",
    "    !git clone https://$GITHUB_AUTH@github.com/GarrettJenkinson/condor_tensorflow.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vWwEuq1E1gql",
    "outputId": "3d3d74d9-791d-4b88-8b8d-16973b1fe119"
   },
   "outputs": [],
   "source": [
    "# Install source package from GitHub\n",
    "if 'google.colab' in str(get_ipython()):\n",
    "    !pip install --force-reinstall --no-deps --use-feature=in-tree-build condor_tensorflow/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import statements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6Xemf4TAtrJC",
    "outputId": "059ed231-319a-4c49-aae1-abe26289fcf3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version 2.6.0\n",
      "CORAL Ordinal version: 0.1.0-dev\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "from scipy import special\n",
    "import tensorflow_hub as hub\n",
    "import os\n",
    "import json\n",
    "import gzip\n",
    "from urllib.request import urlopen\n",
    "\n",
    "import tensorflow as tf\n",
    "print(\"Tensorflow version\", tf.__version__)\n",
    "\n",
    "import condor_tensorflow as condor\n",
    "print(\"CORAL Ordinal version:\", condor.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rq0mT2yYucrx"
   },
   "source": [
    "## MNIST toy example\n",
    "\n",
    "MNIST is a database of handwritten digits widely utilized in image classification tasks.\n",
    "\n",
    "The originally intended use of the dataset is categorical prediction (recognition of digits), without any ordinal component.  However, since the data are numerical, one could imagine a scenario where ordinal proximity of incorrect predictions to the correct prediction might be beneficial e.g. map coordinates.  Hence we utilize the MNIST dataset and enforce ordinal predictions to demonstrate the improved performance of CONDOR on the ordinal problem, while acknowledging that MNIST is usually more suited to categorical prediction.\n",
    "\n",
    "We begin by setting some core variables required for model building."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "fSOcGJBJG1Tr"
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "### SETTINGS\n",
    "##########################\n",
    "\n",
    "# Hyperparameters\n",
    "random_seed = 1 # Not yet used\n",
    "learning_rate = 0.05\n",
    "batch_size = 128\n",
    "num_epochs = 2\n",
    "\n",
    "# Architecture\n",
    "NUM_CLASSES = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we load the MNIST data and create training, test and validation datasets in a suitable format.  Finally we check the shapes of the data structures containing our MNIST data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NooIWGJbGR2u",
    "outputId": "6d75e93d-f2b4-41dd-aef1-85064ee1de97"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of training images: (55000, 28, 28)\n",
      "Shape of training labels: (55000,)\n",
      "Shape of test images: (10000, 28, 28)\n",
      "Shape of test labels: (10000,)\n",
      "Shape of validation images: (5000, 28, 28)\n",
      "Shape of validation labels: (5000,)\n"
     ]
    }
   ],
   "source": [
    "# Fetch and format the mnist data\n",
    "(mnist_images, mnist_labels), (mnist_images_test, mnist_labels_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Split off a validation dataset for early stopping\n",
    "mnist_images, mnist_images_val, mnist_labels, mnist_labels_val = \\\n",
    "  model_selection.train_test_split(mnist_images, mnist_labels, test_size = 5000, random_state = 1)\n",
    "\n",
    "print(\"Shape of training images:\", mnist_images.shape)\n",
    "print(\"Shape of training labels:\", mnist_labels.shape)\n",
    "\n",
    "print(\"Shape of test images:\", mnist_images_test.shape)\n",
    "print(\"Shape of test labels:\", mnist_labels_test.shape)\n",
    "\n",
    "print(\"Shape of validation images:\", mnist_images_val.shape)\n",
    "print(\"Shape of validation labels:\", mnist_labels_val.shape)\n",
    "\n",
    "# Also rescales to 0-1 range.\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images[..., tf.newaxis] / 255, tf.float32),\n",
    "   tf.cast(mnist_labels, tf.int64)))\n",
    "dataset = dataset.shuffle(1000).batch(batch_size)\n",
    "\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images_test[..., tf.newaxis] / 255, tf.float32),\n",
    "   tf.cast(mnist_labels_test, tf.int64)))\n",
    "#test_dataset = test_dataset.shuffle(1000).batch(batch_size)\n",
    "# Here we do not shuffle the test dataset.\n",
    "test_dataset = test_dataset.batch(batch_size)\n",
    "\n",
    "val_dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images_val[..., tf.newaxis] / 255, tf.float32),\n",
    "   tf.cast(mnist_labels_val, tf.int64)))\n",
    "val_dataset = val_dataset.shuffle(1000).batch(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PAg-gWZED1mR"
   },
   "source": [
    "### Simple MLP model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_9MF55Xz53rx"
   },
   "source": [
    "Now we create a simple multi-layer perceptron model and apply the ordinal output layer required by CONDOR (i.e. a dense layer with 1 unit less than the number of output classes).  Note while we use the example of an MLP model, any categorical neural network architecture could be used.  The version below uses the Sequential API to create the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "p-ozv44W52Ti",
    "outputId": "b8751ccb-e887-44e5-ef71-54e4578d2096"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten (Flatten)            (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 104,905\n",
      "Trainable params: 104,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def create_model(num_classes):\n",
    "  model = tf.keras.Sequential()\n",
    "  model.add(tf.keras.layers.Flatten(input_shape = (28, 28, )))\n",
    "  model.add(tf.keras.layers.Dense(128, activation = \"relu\"))\n",
    "  model.add(tf.keras.layers.Dropout(0.2))\n",
    "  model.add(tf.keras.layers.Dense(32, activation = \"relu\"))\n",
    "  model.add(tf.keras.layers.Dropout(0.1))\n",
    "  # No activation function specified so this will output cumulative logits.\n",
    "  model.add(tf.keras.layers.Dense(num_classes-1))\n",
    "  return model\n",
    "\n",
    "model = create_model(NUM_CLASSES)\n",
    "\n",
    "# Note that the model generates 1 fewer outputs than the number of classes. \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively we could build the model using the Functional API as demonstrated below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1YAftcr9wxTu",
    "outputId": "24548743-3ac5-4dc0-a8c2-3ea83c576477"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 28, 28)]          0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               100480    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 32)                4128      \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 9)                 297       \n",
      "=================================================================\n",
      "Total params: 104,905\n",
      "Trainable params: 104,905\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Or a functional API version\n",
    "def create_model2(num_classes):\n",
    "  inputs = tf.keras.Input(shape = (28, 28, ))\n",
    "\n",
    "  x = tf.keras.layers.Flatten()(inputs)\n",
    "  x = tf.keras.layers.Dense(128, activation = \"relu\")(x)\n",
    "  x = tf.keras.layers.Dropout(0.2)(x)\n",
    "  x = tf.keras.layers.Dense(32, activation = \"relu\")(x)\n",
    "  x = tf.keras.layers.Dropout(0.1)(x)\n",
    "  # No activation function specified so this will output cumulative logits.\n",
    "  outputs = tf.keras.layers.Dense(num_classes-1)(x)\n",
    "\n",
    "  model = tf.keras.Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "  return model\n",
    "\n",
    "model = create_model2(NUM_CLASSES)\n",
    "\n",
    "# Note that the model generates 1 fewer outputs than the number of classes. \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compile the model using CONDOR's SparseCondorOrdinalCrossEntropy as the loss function.  This is the key component of the CONDOR method, which enables ordinal prediction with rank consistency.  The other metrics provided by CONDOR enable assessment of CONDOR's performance on the ordinal prediction problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "AUIADdPeF2w6"
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate),\n",
    "              loss = condor.SparseCondorOrdinalCrossEntropy(),\n",
    "              metrics = [condor.SparseOrdinalEarthMoversDistance(),\n",
    "                         condor.SparseOrdinalMeanAbsoluteError()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "as_rVDyAJurK",
    "outputId": "01ec32f4-d1b4-424c-bf6e-8524643d6c14"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "430/430 [==============================] - 6s 9ms/step - loss: 3.4061 - earth_movers_distance_labels: 1.8034 - mean_absolute_error_labels: 1.2779 - val_loss: 2.0726 - val_earth_movers_distance_labels: 1.3388 - val_mean_absolute_error_labels: 0.8356\n",
      "Epoch 2/5\n",
      "430/430 [==============================] - 3s 8ms/step - loss: 2.5324 - earth_movers_distance_labels: 1.5318 - mean_absolute_error_labels: 1.0595 - val_loss: 2.0421 - val_earth_movers_distance_labels: 1.4080 - val_mean_absolute_error_labels: 0.7882\n",
      "Epoch 3/5\n",
      "430/430 [==============================] - 4s 8ms/step - loss: 2.6050 - earth_movers_distance_labels: 1.5678 - mean_absolute_error_labels: 1.0903 - val_loss: 2.0860 - val_earth_movers_distance_labels: 1.4507 - val_mean_absolute_error_labels: 0.7948\n",
      "Epoch 4/5\n",
      "430/430 [==============================] - 4s 8ms/step - loss: 2.6203 - earth_movers_distance_labels: 1.5668 - mean_absolute_error_labels: 1.0941 - val_loss: 2.0929 - val_earth_movers_distance_labels: 1.4451 - val_mean_absolute_error_labels: 0.7918\n",
      "Epoch 5/5\n",
      "430/430 [==============================] - 3s 8ms/step - loss: 2.5828 - earth_movers_distance_labels: 1.5468 - mean_absolute_error_labels: 1.0733 - val_loss: 2.0716 - val_earth_movers_distance_labels: 1.4041 - val_mean_absolute_error_labels: 0.8470\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# This takes about 5 minutes on CPU, 2.5 minutes on GPU.\n",
    "history = model.fit(dataset, epochs = 5, validation_data = val_dataset,\n",
    "                    callbacks = [tf.keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i--Xhzu8D6Mb"
   },
   "source": [
    "### Test set evaluation\n",
    "Now we can evaluate performance on the MNIST test dataset we created previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "epv3NVRmJ1gt",
    "outputId": "02dc7b3c-68a3-4d0c-d367-d6dbb6923250"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79/79 [==============================] - 1s 8ms/step - loss: 2.0195 - earth_movers_distance_labels: 1.3871 - mean_absolute_error_labels: 0.7875\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[2.0194995403289795, 1.3870970010757446, 0.7875000238418579]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate on test dataset.\n",
    "model.evaluate(test_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pfaK4doXvcS"
   },
   "source": [
    "### Cumulative logits to probabilities\n",
    "\n",
    "Note that the output layer naturally outputs cumulative logit values.  These can be  subsequently converted to probability estimates for each ordinal label utilizing the condor.ordinal_softmax() function.  These probababilities can then be used to calculate other metrics like accuracy or mean absolute error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "5MAi4QxZyA2_",
    "outputId": "c856b16a-a015-4c65-a32c-b0d897120d91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict on test dataset\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.673004e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000985</td>\n",
       "      <td>0.018463</td>\n",
       "      <td>0.000520</td>\n",
       "      <td>0.004716</td>\n",
       "      <td>0.001831</td>\n",
       "      <td>5.641987e-01</td>\n",
       "      <td>4.133856e-02</td>\n",
       "      <td>3.679013e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.344650e-07</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.620271</td>\n",
       "      <td>0.378648</td>\n",
       "      <td>0.000021</td>\n",
       "      <td>0.000364</td>\n",
       "      <td>0.000365</td>\n",
       "      <td>5.652808e-06</td>\n",
       "      <td>3.239568e-04</td>\n",
       "      <td>8.392280e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.898683e-01</td>\n",
       "      <td>0.478036</td>\n",
       "      <td>0.039226</td>\n",
       "      <td>0.034971</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.049234</td>\n",
       "      <td>0.032079</td>\n",
       "      <td>5.306434e-03</td>\n",
       "      <td>4.843163e-02</td>\n",
       "      <td>1.033208e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2.898683e-01</td>\n",
       "      <td>0.478036</td>\n",
       "      <td>0.039226</td>\n",
       "      <td>0.034971</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.049234</td>\n",
       "      <td>0.032079</td>\n",
       "      <td>5.306434e-03</td>\n",
       "      <td>4.843163e-02</td>\n",
       "      <td>1.033208e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.172602e-03</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.012016</td>\n",
       "      <td>0.918288</td>\n",
       "      <td>0.000208</td>\n",
       "      <td>0.005483</td>\n",
       "      <td>5.215406e-08</td>\n",
       "      <td>7.450581e-09</td>\n",
       "      <td>6.283215e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2.898683e-01</td>\n",
       "      <td>0.478036</td>\n",
       "      <td>0.039226</td>\n",
       "      <td>0.034971</td>\n",
       "      <td>0.012516</td>\n",
       "      <td>0.049234</td>\n",
       "      <td>0.032079</td>\n",
       "      <td>5.306434e-03</td>\n",
       "      <td>4.843163e-02</td>\n",
       "      <td>1.033208e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3.075600e-05</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.003818</td>\n",
       "      <td>0.970375</td>\n",
       "      <td>0.000006</td>\n",
       "      <td>0.000702</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.506757e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6.659031e-04</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001104</td>\n",
       "      <td>0.043011</td>\n",
       "      <td>0.011639</td>\n",
       "      <td>0.078128</td>\n",
       "      <td>0.051977</td>\n",
       "      <td>2.138722e-02</td>\n",
       "      <td>2.827402e-01</td>\n",
       "      <td>5.093479e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5.411088e-02</td>\n",
       "      <td>0.000028</td>\n",
       "      <td>0.021621</td>\n",
       "      <td>0.107860</td>\n",
       "      <td>0.054946</td>\n",
       "      <td>0.203791</td>\n",
       "      <td>0.148065</td>\n",
       "      <td>1.030344e-02</td>\n",
       "      <td>2.698844e-01</td>\n",
       "      <td>1.293912e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9.894371e-06</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000049</td>\n",
       "      <td>0.009641</td>\n",
       "      <td>0.002023</td>\n",
       "      <td>0.001249</td>\n",
       "      <td>0.003876</td>\n",
       "      <td>5.451417e-02</td>\n",
       "      <td>3.022850e-03</td>\n",
       "      <td>9.256147e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5         6  \\\n",
       "0  4.673004e-05  0.000000  0.000985  0.018463  0.000520  0.004716  0.001831   \n",
       "1  8.344650e-07  0.000000  0.620271  0.378648  0.000021  0.000364  0.000365   \n",
       "2  2.898683e-01  0.478036  0.039226  0.034971  0.012516  0.049234  0.032079   \n",
       "3  2.898683e-01  0.478036  0.039226  0.034971  0.012516  0.049234  0.032079   \n",
       "4  1.172602e-03  0.000000  0.000000  0.012016  0.918288  0.000208  0.005483   \n",
       "5  2.898683e-01  0.478036  0.039226  0.034971  0.012516  0.049234  0.032079   \n",
       "6  3.075600e-05  0.000000  0.000000  0.003818  0.970375  0.000006  0.000702   \n",
       "7  6.659031e-04  0.000000  0.001104  0.043011  0.011639  0.078128  0.051977   \n",
       "8  5.411088e-02  0.000028  0.021621  0.107860  0.054946  0.203791  0.148065   \n",
       "9  9.894371e-06  0.000000  0.000049  0.009641  0.002023  0.001249  0.003876   \n",
       "\n",
       "              7             8             9  \n",
       "0  5.641987e-01  4.133856e-02  3.679013e-01  \n",
       "1  5.652808e-06  3.239568e-04  8.392280e-07  \n",
       "2  5.306434e-03  4.843163e-02  1.033208e-02  \n",
       "3  5.306434e-03  4.843163e-02  1.033208e-02  \n",
       "4  5.215406e-08  7.450581e-09  6.283215e-02  \n",
       "5  5.306434e-03  4.843163e-02  1.033208e-02  \n",
       "6  0.000000e+00  0.000000e+00  2.506757e-02  \n",
       "7  2.138722e-02  2.827402e-01  5.093479e-01  \n",
       "8  1.030344e-02  2.698844e-01  1.293912e-01  \n",
       "9  5.451417e-02  3.022850e-03  9.256147e-01  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Predict on test dataset\")\n",
    "\n",
    "# Note that these are ordinal (cumulative) logits, not probabilities or regular logits.\n",
    "ordinal_logits = model.predict(test_dataset)\n",
    "\n",
    "# Convert from logits to label probabilities. This is initially a tensorflow tensor.\n",
    "tensor_probs = condor.ordinal_softmax(ordinal_logits)\n",
    "\n",
    "# Convert the tensor into a pandas dataframe.\n",
    "probs_df = pd.DataFrame(tensor_probs.numpy())\n",
    "\n",
    "probs_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice above that the probability distribution for each observation is unimodal, which is what we want for an ordinal outcome variable.\n",
    "\n",
    "We can also confirm that our probabilities sum to 1 as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CuFGIJZvymSe",
    "outputId": "846f85cb-331d-4e9d-e82a-c3992ba7b1a8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       1.0\n",
       "1       1.0\n",
       "2       1.0\n",
       "3       1.0\n",
       "4       1.0\n",
       "       ... \n",
       "9995    1.0\n",
       "9996    1.0\n",
       "9997    1.0\n",
       "9998    1.0\n",
       "9999    1.0\n",
       "Length: 10000, dtype: float32"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check that probabilities all sum to 1 - looks good!\n",
    "probs_df.sum(axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZmVlStOItF8"
   },
   "source": [
    "### Label prediction\n",
    "\n",
    "Using the probabilities generated, we can predict labels for the MNIST images.  Here we demonstrate two manners of calculating predicted labels. First we can simply select the label with the highest probability:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "mA344vSf782T",
    "outputId": "442ae806-a019-49c8-80ce-6d37fbc40484"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 2, 1, ..., 4, 8, 6], dtype=int64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probs to labels\n",
    "labels = probs_df.idxmax(axis = 1)\n",
    "labels.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use these labels to calculate the accuracy of our predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "G0Bbs1tb8uh8",
    "outputId": "c74f210e-4801-4b89-f6d9-1570edb30e0f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5851"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(labels == mnist_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "xAUaY171_c22",
    "outputId": "bdfbd060-0e8c-47df-e0d9-2048cdbd0e1a"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.999953</td>\n",
       "      <td>0.999953</td>\n",
       "      <td>0.998968</td>\n",
       "      <td>0.980505</td>\n",
       "      <td>0.979985</td>\n",
       "      <td>0.975269</td>\n",
       "      <td>0.973439</td>\n",
       "      <td>0.409240</td>\n",
       "      <td>3.679013e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.379728</td>\n",
       "      <td>0.001080</td>\n",
       "      <td>0.001059</td>\n",
       "      <td>0.000695</td>\n",
       "      <td>0.000330</td>\n",
       "      <td>0.000325</td>\n",
       "      <td>8.392232e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.710132</td>\n",
       "      <td>0.232096</td>\n",
       "      <td>0.192870</td>\n",
       "      <td>0.157899</td>\n",
       "      <td>0.145383</td>\n",
       "      <td>0.096149</td>\n",
       "      <td>0.064070</td>\n",
       "      <td>0.058764</td>\n",
       "      <td>1.033208e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.710132</td>\n",
       "      <td>0.232096</td>\n",
       "      <td>0.192870</td>\n",
       "      <td>0.157899</td>\n",
       "      <td>0.145383</td>\n",
       "      <td>0.096149</td>\n",
       "      <td>0.064070</td>\n",
       "      <td>0.058764</td>\n",
       "      <td>1.033208e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.998827</td>\n",
       "      <td>0.998827</td>\n",
       "      <td>0.998827</td>\n",
       "      <td>0.986811</td>\n",
       "      <td>0.068523</td>\n",
       "      <td>0.068315</td>\n",
       "      <td>0.062832</td>\n",
       "      <td>0.062832</td>\n",
       "      <td>6.283212e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.710132</td>\n",
       "      <td>0.232096</td>\n",
       "      <td>0.192870</td>\n",
       "      <td>0.157899</td>\n",
       "      <td>0.145383</td>\n",
       "      <td>0.096149</td>\n",
       "      <td>0.064070</td>\n",
       "      <td>0.058764</td>\n",
       "      <td>1.033208e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.999969</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>0.999969</td>\n",
       "      <td>0.996151</td>\n",
       "      <td>0.025776</td>\n",
       "      <td>0.025770</td>\n",
       "      <td>0.025068</td>\n",
       "      <td>0.025068</td>\n",
       "      <td>2.506755e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.999334</td>\n",
       "      <td>0.999334</td>\n",
       "      <td>0.998230</td>\n",
       "      <td>0.955219</td>\n",
       "      <td>0.943580</td>\n",
       "      <td>0.865452</td>\n",
       "      <td>0.813475</td>\n",
       "      <td>0.792088</td>\n",
       "      <td>5.093479e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.945889</td>\n",
       "      <td>0.945861</td>\n",
       "      <td>0.924241</td>\n",
       "      <td>0.816381</td>\n",
       "      <td>0.761435</td>\n",
       "      <td>0.557644</td>\n",
       "      <td>0.409579</td>\n",
       "      <td>0.399276</td>\n",
       "      <td>1.293911e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999990</td>\n",
       "      <td>0.999941</td>\n",
       "      <td>0.990300</td>\n",
       "      <td>0.988277</td>\n",
       "      <td>0.987028</td>\n",
       "      <td>0.983152</td>\n",
       "      <td>0.928638</td>\n",
       "      <td>9.256148e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6  \\\n",
       "0  0.999953  0.999953  0.998968  0.980505  0.979985  0.975269  0.973439   \n",
       "1  0.999999  0.999999  0.379728  0.001080  0.001059  0.000695  0.000330   \n",
       "2  0.710132  0.232096  0.192870  0.157899  0.145383  0.096149  0.064070   \n",
       "3  0.710132  0.232096  0.192870  0.157899  0.145383  0.096149  0.064070   \n",
       "4  0.998827  0.998827  0.998827  0.986811  0.068523  0.068315  0.062832   \n",
       "5  0.710132  0.232096  0.192870  0.157899  0.145383  0.096149  0.064070   \n",
       "6  0.999969  0.999969  0.999969  0.996151  0.025776  0.025770  0.025068   \n",
       "7  0.999334  0.999334  0.998230  0.955219  0.943580  0.865452  0.813475   \n",
       "8  0.945889  0.945861  0.924241  0.816381  0.761435  0.557644  0.409579   \n",
       "9  0.999990  0.999990  0.999941  0.990300  0.988277  0.987028  0.983152   \n",
       "\n",
       "          7             8  \n",
       "0  0.409240  3.679013e-01  \n",
       "1  0.000325  8.392232e-07  \n",
       "2  0.058764  1.033208e-02  \n",
       "3  0.058764  1.033208e-02  \n",
       "4  0.062832  6.283212e-02  \n",
       "5  0.058764  1.033208e-02  \n",
       "6  0.025068  2.506755e-02  \n",
       "7  0.792088  5.093479e-01  \n",
       "8  0.399276  1.293911e-01  \n",
       "9  0.928638  9.256148e-01  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compare to logit-based cumulative probs\n",
    "cum_probs = pd.DataFrame(ordinal_logits).apply(special.expit).cumprod(axis=1)\n",
    "cum_probs.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQ67xh03JOii"
   },
   "source": [
    "Secondly we utilize the method of label prediction proposed by Cao et al in their CORAL method.  Here we choose the label with the highest probability when Pr(Y > label) > 50%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "kNKZU84UBE7s",
    "outputId": "0702f534-b2bc-4142-ac19-d5baccffedbd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    7\n",
       "1    2\n",
       "2    1\n",
       "3    1\n",
       "4    4\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the labels using the style of Cao et al.\n",
    "labels2 = cum_probs.apply(lambda x: x > 0.5).sum(axis = 1)\n",
    "labels2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5f-7BfbYBqSN",
    "outputId": "66e3a654-1a23-4583-c51a-95fa8b52549b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5767"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the accuracy of these labels? \n",
    "np.mean(labels2 == mnist_labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AGlY8HecDFPO",
    "outputId": "d6569bb0-a815-4abc-8b04-da8e19356124"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8274"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# More often than not these are the same, but still a lot of discrepancy.\n",
    "np.mean(labels == labels2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OemBBdUmI6TI",
    "outputId": "d800914f-4a56-4e19-d39b-deb1d79ee7da"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean absolute label error version 1: 0.902\n",
      "Mean absolute label error version 2: 0.7875\n"
     ]
    }
   ],
   "source": [
    "print(\"Mean absolute label error version 1:\", np.mean(np.abs(labels - mnist_labels_test)))\n",
    "print(\"Mean absolute label error version 2:\", np.mean(np.abs(labels2 - mnist_labels_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JCnAaNG_GSTB",
    "outputId": "f4063844-97cd-47cb-c00e-dbfebef48534"
   },
   "outputs": [],
   "source": [
    "mnist_labels_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DpYEPQI6-gW"
   },
   "source": [
    "### Importance weights customization\n",
    "\n",
    "A quick example to show how the importance weights can be customized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "STwPQdgHNne4",
    "outputId": "2e32e75d-8e98-4b7e-8046-2d57d67e906a"
   },
   "outputs": [],
   "source": [
    "model = create_model(num_classes = NUM_CLASSES)\n",
    "model.summary()\n",
    "\n",
    "# We have num_classes - 1 outputs (cumulative logits), so there are 9 elements\n",
    "# in the importance vector to customize.\n",
    "importance_weights = [1., 1., 0.5, 0.5, 0.5, 1., 1., 0.1, 0.1]\n",
    "loss_fn = condor.SparseCondorOrdinalCrossEntropy(importance_weights = importance_weights)\n",
    "\n",
    "model.compile(tf.keras.optimizers.Adam(learning_rate = learning_rate), loss = loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73UOIams7TI_",
    "outputId": "881954e6-01bc-4f31-a1a8-459d07ad98a3"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(dataset, epochs = num_epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fJSm-gbwxTKt"
   },
   "source": [
    "## Amazon reviews and 5-star ratings\n",
    "\n",
    "Amazon review data via https://nijianmo.github.io/amazon/index.html#subsets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VSdEFFwfoZL_"
   },
   "outputs": [],
   "source": [
    "!curl -o Prime_Pantry_5.json.gz http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Prime_Pantry_5.json.gz "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-vHiq67ioiUa",
    "outputId": "3e9352a8-c6f0-4acb-9d14-5aa3b73a5f6a"
   },
   "outputs": [],
   "source": [
    "data = []\n",
    "with gzip.open('Prime_Pantry_5.json.gz') as f:\n",
    "    for l in f:\n",
    "        data.append(json.loads(l.strip()))\n",
    "\n",
    "df = pd.DataFrame.from_dict(data)\n",
    "df = df[['overall', 'reviewText']]\n",
    "\n",
    "# There is a large amount of duplicate text in here, possibly due to paid/fraudulent reviews.\n",
    "df.drop_duplicates(\"reviewText\", inplace = True)\n",
    "\n",
    "# Some of the text is blank, which causes an obscure error about floating point conversion.\n",
    "df.dropna(inplace = True)\n",
    "\n",
    "print(len(df))\n",
    "print(df.head())\n",
    "\n",
    "outcome_col = \"overall\"\n",
    "text_col = \"reviewText\"\n",
    "\n",
    "# We subtract the minimum value from the outcomes so that they start at 0.\n",
    "df[outcome_col] = df[outcome_col].values - df[outcome_col].min()\n",
    "\n",
    "print(\"\\n\", df.overall.value_counts())\n",
    "\n",
    "# TODO: define automatically based on the number of unique values in the outcome variable.\n",
    "num_classes = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "MrGlqgv7tv0A",
    "outputId": "bc47e862-ff48-4d0e-bcc4-e3e24951ad12"
   },
   "outputs": [],
   "source": [
    "# Train/Test split\n",
    "text_train, text_test, labels_train, labels_test = \\\n",
    "  train_test_split(df[text_col].values, df[outcome_col].values, test_size = 10000, random_state = 1)\n",
    "\n",
    "print(\"Training text shape:\", text_train.shape)\n",
    "print(\"Training labels shape:\", labels_train.shape)\n",
    "print(\"Testing text shape:\", text_test.shape)\n",
    "print(\"Testing labels shape:\", labels_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eF48EdMZuTxk"
   },
   "source": [
    "### Universal Sentence Encoder model (minimal code changes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "eOqeIkoJuWcD",
    "outputId": "0ebfc6f3-8a36-4f7d-ad7e-4f712fb147ce"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# This takes 20 - 30 seconds.\n",
    "\n",
    "# Clear our GPU memory to stay efficient.\n",
    "tf.keras.backend.clear_session()\n",
    "\n",
    "input_text = tf.keras.layers.Input(shape = [], dtype = tf.string, name = 'input_text')\n",
    "\n",
    "model_url = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\")\n",
    "\n",
    "base_model = hub.KerasLayer(model_url, input_shape = [],\n",
    "                            dtype = tf.string,\n",
    "                            trainable = False)\n",
    "                            \n",
    "embedded = base_model(input_text)\n",
    "\n",
    "x = tf.keras.layers.Dense(64, activation = 'relu')(embedded)\n",
    "x = tf.keras.layers.Dropout(0.1)(x)\n",
    "output =tf.keras.layers.Dense(num_classes-1)(x) \n",
    "\n",
    "model = tf.keras.Model(inputs = input_text, outputs = output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HmMr5L_u_o64"
   },
   "outputs": [],
   "source": [
    "model.compile(loss = condor.SparseCondorOrdinalCrossEntropy(),\n",
    "              metrics = [condor.SparseOrdinalEarthMoversDistance(),\n",
    "                         condor.SparseOrdinalMeanAbsoluteError()],\n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OwNLAJg33tc4",
    "outputId": "c96a180e-cc73-458f-bad5-0e1ed30040b2"
   },
   "outputs": [],
   "source": [
    "# Encode a test string and take a look at the first ten dimensions.\n",
    "base_model(np.array([\"test_string\"])).numpy()[0, :10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "85MsPnRTxPEz",
    "outputId": "8d6a8187-7aaa-4148-83ca-2b582b23e972"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "history = model.fit(x = text_train,\n",
    "                    y = labels_train,\n",
    "                    epochs = 5,\n",
    "                    batch_size = 32, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = [tf.keras.callbacks.EarlyStopping(patience = 2,\n",
    "                                                                  min_delta = 0.001,\n",
    "                                                                  restore_best_weights = True)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CXFi2qXPlehY"
   },
   "source": [
    "#### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3ocFVWnldzF"
   },
   "outputs": [],
   "source": [
    "# For comparison, CORAL achieves loss 0.7962, MAE 0.3195\n",
    "model.evaluate(text_test, labels_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tqSBmDgXxkFb"
   },
   "outputs": [],
   "source": [
    "# Generate predictions - initially these are cumulative logits.\n",
    "preds = model.predict(text_test)\n",
    "print(preds)\n",
    "# Convert cumulative logits to probabilities for each class aka rank or label.\n",
    "probs = pd.DataFrame(condor.ordinal_softmax(preds).numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KvD7ak27yA26"
   },
   "outputs": [],
   "source": [
    "print(probs.head(10))\n",
    "print(labels_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eNYR4nOjDJt_"
   },
   "source": [
    "#### Evaluate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dZzpmNSGxmJv"
   },
   "outputs": [],
   "source": [
    "# Evaluate accuracy and mean absolute error\n",
    "labels_v1 = probs.idxmax(axis = 1)\n",
    "print(\"Accuracy of label version 1:\", np.mean(labels_v1 == labels_test))\n",
    "\n",
    "# Compare to logit-based cumulative probs\n",
    "cum_probs = pd.DataFrame(preds).apply(special.expit).cumprod(axis=1)\n",
    "# Calculate the labels using the style of Cao et al.\n",
    "labels_v2 = cum_probs.apply(lambda x: x > 0.5).sum(axis = 1)\n",
    "print(\"Accuracy of label version 2:\", np.mean(labels_v2 == labels_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XYKpSMosDMuq"
   },
   "source": [
    "#### Evaluate mean absolute label error\n",
    "\n",
    "This is effectively an ordinal version of 1 - accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJZNCzwfGqC0"
   },
   "outputs": [],
   "source": [
    "# These do not correspond with what we get from the model evaluation. Something must be off in one of these.\n",
    "print(\"Mean absolute label error version 1:\", np.mean(np.abs(labels_v1 - labels_test)))\n",
    "print(\"Mean absolute label error version 2:\", np.mean(np.abs(labels_v2 - labels_test)))\n",
    "\n",
    "print(\"Root mean squared label error version 1:\", np.sqrt(np.mean(np.square(labels_v1 - labels_test))))\n",
    "print(\"Root mean squared label error version 2:\", np.sqrt(np.mean(np.square(labels_v2 - labels_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ddSncBedI37-"
   },
   "outputs": [],
   "source": [
    "# Review how absolute error is calculated for ordinal labels:\n",
    "pd.DataFrame({\"true\": labels_test, \"pred_v2\": labels_v1, \"abs\": labels_v2 - labels_test}).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dUyJw-20B2AU"
   },
   "source": [
    "### Universal Sentence Encoder model (speed up using encodings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnCkqHv5CA_x"
   },
   "source": [
    "The \"Sparse\" versions of the CONDOR API are convenient and require minimal code changes. However there is a performance overhead compared to if we pre-encode the labels using CONDORs ordinal encoder. The sparse API is basically encoding on the fly inside the training loop. \n",
    "\n",
    "Also as we will see later, the labels do not always come encoded as 0,1,...,K-1. In this case, using the CondorOrdinalEncoder will help transform labels into ordinal-ready values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rq1aA2ffB2Ai"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# pre-encoding runs very fast so the savings later are worth it\n",
    "enc = condor.CondorOrdinalEncoder(nclasses=num_classes)\n",
    "enc_labs_train = enc.fit_transform(labels_train)\n",
    "enc_labs_test = enc.transform(labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahRP7kkPB2Ai"
   },
   "outputs": [],
   "source": [
    "# Note the lack of \"Sparse\" in the condor functions here\n",
    "model.compile(loss = condor.CondorOrdinalCrossEntropy(),\n",
    "              metrics = [condor.OrdinalEarthMoversDistance(),\n",
    "                         condor.OrdinalMeanAbsoluteError()],\n",
    "              optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fb2NU9kVB2Ai"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "# note the encoded labels are passed to the fit now\n",
    "history = model.fit(x = text_train,\n",
    "                    y = enc_labs_train,\n",
    "                    epochs = 5,\n",
    "                    batch_size = 32, \n",
    "                    validation_split = 0.2,\n",
    "                    callbacks = [tf.keras.callbacks.EarlyStopping(patience = 2,\n",
    "                                                                  min_delta = 0.001,\n",
    "                                                                  restore_best_weights = True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OWBl7PuZB2Ai"
   },
   "outputs": [],
   "source": [
    "model.evaluate(text_test, enc_labs_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xt9mAOqvE4Ep"
   },
   "source": [
    "#### More examples of label encoding capabilities\n",
    "Here we demo the features of the ordinal encoder.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eGcYAoi0E_FI"
   },
   "outputs": [],
   "source": [
    "# Here the ordinal encoder figures out how many classes there are automatically\n",
    "# and orders them in the default sklearn OrdinalEncoder fashion \n",
    "# (i.e., alphabetically here)\n",
    "labels = np.array(['a','b','c','d','e'])\n",
    "enc_labs = condor.CondorOrdinalEncoder().fit_transform(labels)\n",
    "print(enc_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9YuSAf1wFgAq"
   },
   "outputs": [],
   "source": [
    "# Here the ordinal encoder figures out how many classes there are automatically\n",
    "# and orders them in the default sklearn OrdinalEncoder fashion \n",
    "# (i.e., alphabetically here). This time it is dealing with a basic list.\n",
    "labels = ['a','b','c','d','e']\n",
    "enc_labs = condor.CondorOrdinalEncoder().fit_transform(labels)\n",
    "\n",
    "print(enc_labs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FkzccbsjFwdv"
   },
   "outputs": [],
   "source": [
    "# Here we wish to specify the order to be different from alphabetical. Note\n",
    "# this would also allow \"missing\" categories to be included in proper order.\n",
    "labels = ['low','med','high']\n",
    "enc = condor.CondorOrdinalEncoder(categories=[['low', 'med', 'high']])\n",
    "enc_labs = enc.fit_transform(labels)\n",
    "\n",
    "print(enc_labs)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CONDOR TensorFlow demo.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
