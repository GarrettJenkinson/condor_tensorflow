{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CONDOR tensorflow implementation for ordinal regression with deep neural networks. About CONDOR, short for CONDitional Ordinal Regression, is a method for ordinal regression with deep neural networks, which addresses the rank inconsistency issue of other ordinal regression frameworks. It is compatible with any state-of-the-art deep neural network architecture, requiring only modification of the output layer, the labels, the loss function. We also have condor implemented for pytorch . This package includes: Ordinal tensorflow loss function: CondorOrdinalCrossEntropy Ordinal tensorflow error metric: OrdinalMeanAbsoluteError Ordinal tensorflow error metric: OrdinalEarthMoversDistance Ordinal tensorflow sparse loss function: CondorSparseOrdinalCrossEntropy Ordinal tensorflow sparse error metric: SparseOrdinalMeanAbsoluteError Ordinal tensorflow sparse error metric: SparseOrdinalEarthMoversDistance Ordinal tensorflow activation function: ordinal_softmax Ordinal sklearn label encoder: CondorOrdinalEncoder Cite as If you use CONDOR as part of your workflow in a scientific publication, please consider citing the CONDOR repository with the following DOI: TBD publication @article{condor2021, title = \"TBD\", journal = \"TBD\", volume = \"TBD\", pages = \"TBD\", year = \"TBD\", issn = \"TBD\", doi = \"TBD\", url = \"TBD\", author = \"Garrett Jenkinson\", keywords = \"Deep learning, Ordinal regression, neural networks, Machine learning, Biometrics\" }","title":"Home"},{"location":"#about","text":"CONDOR, short for CONDitional Ordinal Regression, is a method for ordinal regression with deep neural networks, which addresses the rank inconsistency issue of other ordinal regression frameworks. It is compatible with any state-of-the-art deep neural network architecture, requiring only modification of the output layer, the labels, the loss function. We also have condor implemented for pytorch . This package includes: Ordinal tensorflow loss function: CondorOrdinalCrossEntropy Ordinal tensorflow error metric: OrdinalMeanAbsoluteError Ordinal tensorflow error metric: OrdinalEarthMoversDistance Ordinal tensorflow sparse loss function: CondorSparseOrdinalCrossEntropy Ordinal tensorflow sparse error metric: SparseOrdinalMeanAbsoluteError Ordinal tensorflow sparse error metric: SparseOrdinalEarthMoversDistance Ordinal tensorflow activation function: ordinal_softmax Ordinal sklearn label encoder: CondorOrdinalEncoder","title":"About"},{"location":"#cite-as","text":"If you use CONDOR as part of your workflow in a scientific publication, please consider citing the CONDOR repository with the following DOI: TBD publication @article{condor2021, title = \"TBD\", journal = \"TBD\", volume = \"TBD\", pages = \"TBD\", year = \"TBD\", issn = \"TBD\", doi = \"TBD\", url = \"TBD\", author = \"Garrett Jenkinson\", keywords = \"Deep learning, Ordinal regression, neural networks, Machine learning, Biometrics\" }","title":"Cite as"},{"location":"CHANGELOG/","text":"Release Notes The changelog for the current development version is available at https://github.com/GarrettJenkinson/condor_tensorflow/blob/main/docs/CHANGELOG.md . 1.0.0 (10/14/2021) Downloads Source code (zip) Source code (tar.gz) New Features First release. Changes First release. Bug Fixes First release.","title":"Changelog"},{"location":"CHANGELOG/#release-notes","text":"The changelog for the current development version is available at https://github.com/GarrettJenkinson/condor_tensorflow/blob/main/docs/CHANGELOG.md .","title":"Release Notes"},{"location":"CHANGELOG/#100-10142021","text":"","title":"1.0.0 (10/14/2021)"},{"location":"CHANGELOG/#downloads","text":"Source code (zip) Source code (tar.gz)","title":"Downloads"},{"location":"CHANGELOG/#new-features","text":"First release.","title":"New Features"},{"location":"CHANGELOG/#changes","text":"First release.","title":"Changes"},{"location":"CHANGELOG/#bug-fixes","text":"First release.","title":"Bug Fixes"},{"location":"citing/","text":"If you use CONDOR as part of your workflow in a scientific publication, please consider citing the CONDOR repository with the following DOI: Jenkinson, Khezeli, Oliver, Kalantari, Klee. Universally rank consistent ordinal regression in neural networks, arXiv:2110.07470, 2021. @article{condor2021, title = \"Universally rank consistent ordinal regression in neural networks\", journal = \"arXiv\", volume = \"2110.07470\", year = \"2021\", url = \"https://arxiv.org/abs/2110.07470\", author = \"Garrett Jenkinson and Kia Khezeli and Gavin R. Oliver and John Kalantari and Eric W. Klee\", keywords = \"Deep learning, Ordinal regression, neural networks, Machine learning, Biometrics\" }","title":"Citing"},{"location":"installation/","text":"Installing condor_tensorflow Requirements condor_tensorflow has been tested with the following software and packages: Python == 3.9.6 Tensorflow == 2.4.1 sklearn == 0.24.2 numpy == 1.19.5 (newer versions currently incompatibile with tensorflow) PyPI You can install the latest stable release of condor_tensorflow directly from Python's package index via pip by executing the following code from your command line: pip install condor_tensorflow Latest GitHub Source Code You want to try out the latest features before they go live on PyPI? Install the condor_tensorflow dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/GarrettJenkinson/condor_tensorflow.git Docker This package relies on Python 3.6+, Tensorflow 2.2+, sklearn, and numpy. For convenience we provide a Dockerfile that will build a container with condor_tensorflow as well as its dependencies. This can be used as # Create a docker image docker build -t cpu_tensorflow -f cpu.Dockerfile ./ # run image to serve a jupyter notebook docker run -it -p 8888:8888 --rm cpu_tensorflow # how to run bash inside container (with python that will have deps) docker run -u $(id -u):$(id -g) -it -p 8888:8888 --rm cpu_tensorflow bash Assuming a GPU enabled machine with the NVIDIA drivers installed replace cpu above with gpu .","title":"Installation"},{"location":"installation/#installing-condor_tensorflow","text":"","title":"Installing condor_tensorflow"},{"location":"installation/#requirements","text":"condor_tensorflow has been tested with the following software and packages: Python == 3.9.6 Tensorflow == 2.4.1 sklearn == 0.24.2 numpy == 1.19.5 (newer versions currently incompatibile with tensorflow)","title":"Requirements"},{"location":"installation/#pypi","text":"You can install the latest stable release of condor_tensorflow directly from Python's package index via pip by executing the following code from your command line: pip install condor_tensorflow","title":"PyPI"},{"location":"installation/#latest-github-source-code","text":"You want to try out the latest features before they go live on PyPI? Install the condor_tensorflow dev-version latest development version from the GitHub repository by executing pip install git+git://github.com/GarrettJenkinson/condor_tensorflow.git","title":"Latest GitHub Source Code"},{"location":"installation/#docker","text":"This package relies on Python 3.6+, Tensorflow 2.2+, sklearn, and numpy. For convenience we provide a Dockerfile that will build a container with condor_tensorflow as well as its dependencies. This can be used as # Create a docker image docker build -t cpu_tensorflow -f cpu.Dockerfile ./ # run image to serve a jupyter notebook docker run -it -p 8888:8888 --rm cpu_tensorflow # how to run bash inside container (with python that will have deps) docker run -u $(id -u):$(id -g) -it -p 8888:8888 --rm cpu_tensorflow bash Assuming a GPU enabled machine with the NVIDIA drivers installed replace cpu above with gpu .","title":"Docker"},{"location":"license/","text":"MIT License Copyright (c) 2021 Garrett Jenkinson Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"License"},{"location":"license/#mit-license","text":"Copyright (c) 2021 Garrett Jenkinson Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions: The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software. THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.","title":"MIT License"},{"location":"api_modules/condor_tensorflow.labelencoder/BaseEstimator/","text":"BaseEstimator BaseEstimator() Base class for all estimators in scikit-learn. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. Properties","title":"condor_tensorflow.labelencoder.BaseEstimator"},{"location":"api_modules/condor_tensorflow.labelencoder/BaseEstimator/#baseestimator","text":"BaseEstimator() Base class for all estimators in scikit-learn. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"BaseEstimator"},{"location":"api_modules/condor_tensorflow.labelencoder/BaseEstimator/#methods","text":"get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance.","title":"Methods"},{"location":"api_modules/condor_tensorflow.labelencoder/BaseEstimator/#properties","text":"","title":"Properties"},{"location":"api_modules/condor_tensorflow.labelencoder/CondorOrdinalEncoder/","text":"CondorOrdinalEncoder CondorOrdinalEncoder(nclasses=0, dtype= , kwargs) Base class for all estimators in scikit-learn. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Fit the CondorOrdinalEncoder to X. Parameters X : array-like of shape (n_samples, n_features) The data to determine the categories of each feature. y : None Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Transform X to ordinal arrays. Parameters X : array-like of shape (n_samples, 1) The labels data to encode. Returns X_out : ndarray of shape (n_samples, n_classes-1) Transformed input. Properties","title":"condor_tensorflow.labelencoder.CondorOrdinalEncoder"},{"location":"api_modules/condor_tensorflow.labelencoder/CondorOrdinalEncoder/#condorordinalencoder","text":"CondorOrdinalEncoder(nclasses=0, dtype= , kwargs) Base class for all estimators in scikit-learn. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"CondorOrdinalEncoder"},{"location":"api_modules/condor_tensorflow.labelencoder/CondorOrdinalEncoder/#methods","text":"fit(X, y=None) Fit the CondorOrdinalEncoder to X. Parameters X : array-like of shape (n_samples, n_features) The data to determine the categories of each feature. y : None Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Transform X to ordinal arrays. Parameters X : array-like of shape (n_samples, 1) The labels data to encode. Returns X_out : ndarray of shape (n_samples, n_classes-1) Transformed input.","title":"Methods"},{"location":"api_modules/condor_tensorflow.labelencoder/CondorOrdinalEncoder/#properties","text":"","title":"Properties"},{"location":"api_modules/condor_tensorflow.labelencoder/OrdinalEncoder/","text":"OrdinalEncoder OrdinalEncoder( , categories='auto', dtype= , handle_unknown='error', unknown_value=None)* Encode categorical features as an integer array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. Read more in the :ref:`User Guide <preprocessing_categorical_features>`. .. versionadded:: 0.20 Parameters categories : 'auto' or a list of array-like, default='auto' Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values, and should be sorted in case of numeric values. The used categories can be found in the categories_ attribute. dtype : number type, default np.float64 Desired dtype of output. handle_unknown : {'error', 'use_encoded_value'}, default='error' When set to 'error' an error will be raised in case an unknown categorical feature is present during transform. When set to 'use_encoded_value', the encoded value of unknown categories will be set to the value given for the parameter unknown_value . In :meth: inverse_transform , an unknown category will be denoted as None. .. versionadded:: 0.24 unknown_value : int or np.nan, default=None When the parameter handle_unknown is set to 'use_encoded_value', this parameter is required and will set the encoded value of unknown categories. It has to be distinct from the values used to encode any of the categories in fit . If set to np.nan, the dtype parameter must be a float dtype. .. versionadded:: 0.24 Attributes categories_ : list of arrays The categories of each feature determined during fit (in order of the features in X and corresponding with the output of transform ). This does not include categories that weren't seen during fit . See Also OneHotEncoder : Performs a one-hot encoding of categorical features. LabelEncoder : Encodes target labels with values between 0 and n_classes-1 . Examples Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding. ``` >>> from sklearn.preprocessing import OrdinalEncoder >>> enc = OrdinalEncoder() >>> X = [['Male', 1], ['Female', 3], ['Female', 2]] >>> enc.fit(X) OrdinalEncoder() >>> enc.categories_ [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)] >>> enc.transform([['Female', 3], ['Male', 1]]) array([[0., 2.], [1., 0.]]) >>> enc.inverse_transform([[1, 0], [0, 1]]) array([['Male', 1], ['Female', 2]], dtype=object) ``` Methods fit(X, y=None) Fit the OrdinalEncoder to X. Parameters X : array-like of shape (n_samples, n_features) The data to determine the categories of each feature. y : None Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. inverse_transform(X) Convert the data back to the original representation. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) The transformed data. Returns X_tr : ndarray of shape (n_samples, n_features) Inverse transformed array. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Transform X to ordinal codes. Parameters X : array-like of shape (n_samples, n_features) The data to encode. Returns X_out : ndarray of shape (n_samples, n_features) Transformed input. Properties","title":"condor_tensorflow.labelencoder.OrdinalEncoder"},{"location":"api_modules/condor_tensorflow.labelencoder/OrdinalEncoder/#ordinalencoder","text":"OrdinalEncoder( , categories='auto', dtype= , handle_unknown='error', unknown_value=None)* Encode categorical features as an integer array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. Read more in the :ref:`User Guide <preprocessing_categorical_features>`. .. versionadded:: 0.20 Parameters categories : 'auto' or a list of array-like, default='auto' Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values, and should be sorted in case of numeric values. The used categories can be found in the categories_ attribute. dtype : number type, default np.float64 Desired dtype of output. handle_unknown : {'error', 'use_encoded_value'}, default='error' When set to 'error' an error will be raised in case an unknown categorical feature is present during transform. When set to 'use_encoded_value', the encoded value of unknown categories will be set to the value given for the parameter unknown_value . In :meth: inverse_transform , an unknown category will be denoted as None. .. versionadded:: 0.24 unknown_value : int or np.nan, default=None When the parameter handle_unknown is set to 'use_encoded_value', this parameter is required and will set the encoded value of unknown categories. It has to be distinct from the values used to encode any of the categories in fit . If set to np.nan, the dtype parameter must be a float dtype. .. versionadded:: 0.24 Attributes categories_ : list of arrays The categories of each feature determined during fit (in order of the features in X and corresponding with the output of transform ). This does not include categories that weren't seen during fit . See Also OneHotEncoder : Performs a one-hot encoding of categorical features. LabelEncoder : Encodes target labels with values between 0 and n_classes-1 . Examples Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding. ``` >>> from sklearn.preprocessing import OrdinalEncoder >>> enc = OrdinalEncoder() >>> X = [['Male', 1], ['Female', 3], ['Female', 2]] >>> enc.fit(X) OrdinalEncoder() >>> enc.categories_ [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)] >>> enc.transform([['Female', 3], ['Male', 1]]) array([[0., 2.], [1., 0.]]) >>> enc.inverse_transform([[1, 0], [0, 1]]) array([['Male', 1], ['Female', 2]], dtype=object) ```","title":"OrdinalEncoder"},{"location":"api_modules/condor_tensorflow.labelencoder/OrdinalEncoder/#methods","text":"fit(X, y=None) Fit the OrdinalEncoder to X. Parameters X : array-like of shape (n_samples, n_features) The data to determine the categories of each feature. y : None Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. inverse_transform(X) Convert the data back to the original representation. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) The transformed data. Returns X_tr : ndarray of shape (n_samples, n_features) Inverse transformed array. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Transform X to ordinal codes. Parameters X : array-like of shape (n_samples, n_features) The data to encode. Returns X_out : ndarray of shape (n_samples, n_features) Transformed input.","title":"Methods"},{"location":"api_modules/condor_tensorflow.labelencoder/OrdinalEncoder/#properties","text":"","title":"Properties"},{"location":"api_modules/condor_tensorflow.labelencoder/TransformerMixin/","text":"TransformerMixin TransformerMixin() Mixin class for all transformers in scikit-learn. Methods fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. Properties","title":"condor_tensorflow.labelencoder.TransformerMixin"},{"location":"api_modules/condor_tensorflow.labelencoder/TransformerMixin/#transformermixin","text":"TransformerMixin() Mixin class for all transformers in scikit-learn.","title":"TransformerMixin"},{"location":"api_modules/condor_tensorflow.labelencoder/TransformerMixin/#methods","text":"fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Methods"},{"location":"api_modules/condor_tensorflow.labelencoder/TransformerMixin/#properties","text":"","title":"Properties"},{"location":"api_modules/condor_tensorflow.loss/CondorOrdinalCrossEntropy/","text":"CondorOrdinalCrossEntropy CondorOrdinalCrossEntropy(importance_weights=None, from_type='ordinal_logits', name='ordinal_crossent', kwargs) Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) ``` When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom_training) for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: ```python with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)) ``` Methods call(y_true, y_pred) Invokes the Loss instance. Args: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` Returns: Loss values with the shape `[batch_size, d0, .. dN-1]`. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the config dictionary for a Loss instance. ordinal_loss(logits, levels, importance) Cross-entropy loss function designed for ordinal outcomes. Parameters logits: tf.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. levels: tf.Tensor, shape=(num_samples, num_classes-1) Encoded lables provided by CondorOrdinalEncoder. importance_weights: tf or np array of floats, shape(numclasses-1,) Importance weights for each binary classification task. Returns loss: tf.Tensor, shape=(num_samples,) Loss vector, note that tensorflow will reduce it to a single number automatically. Properties","title":"condor_tensorflow.loss.CondorOrdinalCrossEntropy"},{"location":"api_modules/condor_tensorflow.loss/CondorOrdinalCrossEntropy/#condorordinalcrossentropy","text":"CondorOrdinalCrossEntropy(importance_weights=None, from_type='ordinal_logits', name='ordinal_crossent', kwargs) Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) ``` When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom_training) for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: ```python with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)) ```","title":"CondorOrdinalCrossEntropy"},{"location":"api_modules/condor_tensorflow.loss/CondorOrdinalCrossEntropy/#methods","text":"call(y_true, y_pred) Invokes the Loss instance. Args: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` Returns: Loss values with the shape `[batch_size, d0, .. dN-1]`. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the config dictionary for a Loss instance. ordinal_loss(logits, levels, importance) Cross-entropy loss function designed for ordinal outcomes. Parameters logits: tf.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. levels: tf.Tensor, shape=(num_samples, num_classes-1) Encoded lables provided by CondorOrdinalEncoder. importance_weights: tf or np array of floats, shape(numclasses-1,) Importance weights for each binary classification task. Returns loss: tf.Tensor, shape=(num_samples,) Loss vector, note that tensorflow will reduce it to a single number automatically.","title":"Methods"},{"location":"api_modules/condor_tensorflow.loss/CondorOrdinalCrossEntropy/#properties","text":"","title":"Properties"},{"location":"api_modules/condor_tensorflow.loss/OrdinalEarthMoversDistance/","text":"OrdinalEarthMoversDistance OrdinalEarthMoversDistance(name='earth_movers_distance', kwargs) Computes earth movers distance for ordinal labels. Methods call(y_true, y_pred) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the serializable config of the metric. Properties","title":"condor_tensorflow.loss.OrdinalEarthMoversDistance"},{"location":"api_modules/condor_tensorflow.loss/OrdinalEarthMoversDistance/#ordinalearthmoversdistance","text":"OrdinalEarthMoversDistance(name='earth_movers_distance', kwargs) Computes earth movers distance for ordinal labels.","title":"OrdinalEarthMoversDistance"},{"location":"api_modules/condor_tensorflow.loss/OrdinalEarthMoversDistance/#methods","text":"call(y_true, y_pred) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the serializable config of the metric.","title":"Methods"},{"location":"api_modules/condor_tensorflow.loss/OrdinalEarthMoversDistance/#properties","text":"","title":"Properties"},{"location":"api_modules/condor_tensorflow.loss/SparseCondorOrdinalCrossEntropy/","text":"SparseCondorOrdinalCrossEntropy SparseCondorOrdinalCrossEntropy(importance_weights=None, from_type='ordinal_logits', name='ordinal_crossent', kwargs) Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) ``` When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom_training) for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: ```python with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)) ``` Methods call(y_true, y_pred) Invokes the Loss instance. Args: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` Returns: Loss values with the shape `[batch_size, d0, .. dN-1]`. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the config dictionary for a Loss instance. label_to_levels(label) None ordinal_loss(logits, levels, importance) Cross-entropy loss function designed for ordinal outcomes. Parameters logits: tf.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. levels: tf.Tensor, shape=(num_samples, num_classes-1) Encoded lables provided by CondorOrdinalEncoder. importance_weights: tf or np array of floats, shape(numclasses-1,) Importance weights for each binary classification task. Returns loss: tf.Tensor, shape=(num_samples,) Loss vector, note that tensorflow will reduce it to a single number automatically. Properties","title":"condor_tensorflow.loss.SparseCondorOrdinalCrossEntropy"},{"location":"api_modules/condor_tensorflow.loss/SparseCondorOrdinalCrossEntropy/#sparsecondorordinalcrossentropy","text":"SparseCondorOrdinalCrossEntropy(importance_weights=None, from_type='ordinal_logits', name='ordinal_crossent', kwargs) Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) ``` When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom_training) for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: ```python with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)) ```","title":"SparseCondorOrdinalCrossEntropy"},{"location":"api_modules/condor_tensorflow.loss/SparseCondorOrdinalCrossEntropy/#methods","text":"call(y_true, y_pred) Invokes the Loss instance. Args: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` Returns: Loss values with the shape `[batch_size, d0, .. dN-1]`. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the config dictionary for a Loss instance. label_to_levels(label) None ordinal_loss(logits, levels, importance) Cross-entropy loss function designed for ordinal outcomes. Parameters logits: tf.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. levels: tf.Tensor, shape=(num_samples, num_classes-1) Encoded lables provided by CondorOrdinalEncoder. importance_weights: tf or np array of floats, shape(numclasses-1,) Importance weights for each binary classification task. Returns loss: tf.Tensor, shape=(num_samples,) Loss vector, note that tensorflow will reduce it to a single number automatically.","title":"Methods"},{"location":"api_modules/condor_tensorflow.loss/SparseCondorOrdinalCrossEntropy/#properties","text":"","title":"Properties"},{"location":"api_modules/condor_tensorflow.loss/SparseOrdinalEarthMoversDistance/","text":"SparseOrdinalEarthMoversDistance SparseOrdinalEarthMoversDistance( kwargs) Computes earth movers distance for ordinal labels. Methods call(y_true, y_pred) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: Sparse Labels with values in {0,1,...,num_classes-1} from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the serializable config of the metric. Properties","title":"condor_tensorflow.loss.SparseOrdinalEarthMoversDistance"},{"location":"api_modules/condor_tensorflow.loss/SparseOrdinalEarthMoversDistance/#sparseordinalearthmoversdistance","text":"SparseOrdinalEarthMoversDistance( kwargs) Computes earth movers distance for ordinal labels.","title":"SparseOrdinalEarthMoversDistance"},{"location":"api_modules/condor_tensorflow.loss/SparseOrdinalEarthMoversDistance/#methods","text":"call(y_true, y_pred) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: Sparse Labels with values in {0,1,...,num_classes-1} from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the serializable config of the metric.","title":"Methods"},{"location":"api_modules/condor_tensorflow.loss/SparseOrdinalEarthMoversDistance/#properties","text":"","title":"Properties"},{"location":"api_modules/condor_tensorflow.metrics/OrdinalAccuracy/","text":"OrdinalAccuracy OrdinalAccuracy( args, * kwargs) Computes accuracy for ordinal labels (tolerance is allowed rank distance to be considered 'correct' predictions). Methods add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes accuracy for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"condor_tensorflow.metrics.OrdinalAccuracy"},{"location":"api_modules/condor_tensorflow.metrics/OrdinalAccuracy/#ordinalaccuracy","text":"OrdinalAccuracy( args, * kwargs) Computes accuracy for ordinal labels (tolerance is allowed rank distance to be considered 'correct' predictions).","title":"OrdinalAccuracy"},{"location":"api_modules/condor_tensorflow.metrics/OrdinalAccuracy/#methods","text":"add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes accuracy for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"Methods"},{"location":"api_modules/condor_tensorflow.metrics/OrdinalMeanAbsoluteError/","text":"OrdinalMeanAbsoluteError OrdinalMeanAbsoluteError( args, * kwargs) Computes mean absolute error for ordinal labels. Methods add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"condor_tensorflow.metrics.OrdinalMeanAbsoluteError"},{"location":"api_modules/condor_tensorflow.metrics/OrdinalMeanAbsoluteError/#ordinalmeanabsoluteerror","text":"OrdinalMeanAbsoluteError( args, * kwargs) Computes mean absolute error for ordinal labels.","title":"OrdinalMeanAbsoluteError"},{"location":"api_modules/condor_tensorflow.metrics/OrdinalMeanAbsoluteError/#methods","text":"add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"Methods"},{"location":"api_modules/condor_tensorflow.metrics/SparseOrdinalAccuracy/","text":"SparseOrdinalAccuracy SparseOrdinalAccuracy( args, * kwargs) Computes accuracy for ordinal labels (tolerance is allowed rank distance to be considered 'correct' predictions). Methods add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes accuracy for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"condor_tensorflow.metrics.SparseOrdinalAccuracy"},{"location":"api_modules/condor_tensorflow.metrics/SparseOrdinalAccuracy/#sparseordinalaccuracy","text":"SparseOrdinalAccuracy( args, * kwargs) Computes accuracy for ordinal labels (tolerance is allowed rank distance to be considered 'correct' predictions).","title":"SparseOrdinalAccuracy"},{"location":"api_modules/condor_tensorflow.metrics/SparseOrdinalAccuracy/#methods","text":"add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes accuracy for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"Methods"},{"location":"api_modules/condor_tensorflow.metrics/SparseOrdinalMeanAbsoluteError/","text":"SparseOrdinalMeanAbsoluteError SparseOrdinalMeanAbsoluteError( args, * kwargs) Computes mean absolute error for ordinal labels. Methods add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"condor_tensorflow.metrics.SparseOrdinalMeanAbsoluteError"},{"location":"api_modules/condor_tensorflow.metrics/SparseOrdinalMeanAbsoluteError/#sparseordinalmeanabsoluteerror","text":"SparseOrdinalMeanAbsoluteError( args, * kwargs) Computes mean absolute error for ordinal labels.","title":"SparseOrdinalMeanAbsoluteError"},{"location":"api_modules/condor_tensorflow.metrics/SparseOrdinalMeanAbsoluteError/#methods","text":"add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.activations/","text":"condor_tensorflow version: 0.1.0-dev","title":"condor_tensorflow.activations"},{"location":"api_subpackages/condor_tensorflow.labelencoder/","text":"condor_tensorflow version: 0.1.0-dev TransformerMixin TransformerMixin() Mixin class for all transformers in scikit-learn. Methods fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. Properties OrdinalEncoder OrdinalEncoder( , categories='auto', dtype= , handle_unknown='error', unknown_value=None)* Encode categorical features as an integer array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. Read more in the :ref:`User Guide <preprocessing_categorical_features>`. .. versionadded:: 0.20 Parameters categories : 'auto' or a list of array-like, default='auto' Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values, and should be sorted in case of numeric values. The used categories can be found in the categories_ attribute. dtype : number type, default np.float64 Desired dtype of output. handle_unknown : {'error', 'use_encoded_value'}, default='error' When set to 'error' an error will be raised in case an unknown categorical feature is present during transform. When set to 'use_encoded_value', the encoded value of unknown categories will be set to the value given for the parameter unknown_value . In :meth: inverse_transform , an unknown category will be denoted as None. .. versionadded:: 0.24 unknown_value : int or np.nan, default=None When the parameter handle_unknown is set to 'use_encoded_value', this parameter is required and will set the encoded value of unknown categories. It has to be distinct from the values used to encode any of the categories in fit . If set to np.nan, the dtype parameter must be a float dtype. .. versionadded:: 0.24 Attributes categories_ : list of arrays The categories of each feature determined during fit (in order of the features in X and corresponding with the output of transform ). This does not include categories that weren't seen during fit . See Also OneHotEncoder : Performs a one-hot encoding of categorical features. LabelEncoder : Encodes target labels with values between 0 and n_classes-1 . Examples Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding. ``` >>> from sklearn.preprocessing import OrdinalEncoder >>> enc = OrdinalEncoder() >>> X = [['Male', 1], ['Female', 3], ['Female', 2]] >>> enc.fit(X) OrdinalEncoder() >>> enc.categories_ [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)] >>> enc.transform([['Female', 3], ['Male', 1]]) array([[0., 2.], [1., 0.]]) >>> enc.inverse_transform([[1, 0], [0, 1]]) array([['Male', 1], ['Female', 2]], dtype=object) ``` Methods fit(X, y=None) Fit the OrdinalEncoder to X. Parameters X : array-like of shape (n_samples, n_features) The data to determine the categories of each feature. y : None Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. inverse_transform(X) Convert the data back to the original representation. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) The transformed data. Returns X_tr : ndarray of shape (n_samples, n_features) Inverse transformed array. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Transform X to ordinal codes. Parameters X : array-like of shape (n_samples, n_features) The data to encode. Returns X_out : ndarray of shape (n_samples, n_features) Transformed input. Properties BaseEstimator BaseEstimator() Base class for all estimators in scikit-learn. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. Properties CondorOrdinalEncoder CondorOrdinalEncoder(nclasses=0, dtype= , kwargs) Base class for all estimators in scikit-learn. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ). Methods fit(X, y=None) Fit the CondorOrdinalEncoder to X. Parameters X : array-like of shape (n_samples, n_features) The data to determine the categories of each feature. y : None Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Transform X to ordinal arrays. Parameters X : array-like of shape (n_samples, 1) The labels data to encode. Returns X_out : ndarray of shape (n_samples, n_classes-1) Transformed input. Properties","title":"condor_tensorflow.labelencoder"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#transformermixin","text":"TransformerMixin() Mixin class for all transformers in scikit-learn.","title":"TransformerMixin"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#methods","text":"fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#properties","text":"","title":"Properties"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#ordinalencoder","text":"OrdinalEncoder( , categories='auto', dtype= , handle_unknown='error', unknown_value=None)* Encode categorical features as an integer array. The input to this transformer should be an array-like of integers or strings, denoting the values taken on by categorical (discrete) features. The features are converted to ordinal integers. This results in a single column of integers (0 to n_categories - 1) per feature. Read more in the :ref:`User Guide <preprocessing_categorical_features>`. .. versionadded:: 0.20 Parameters categories : 'auto' or a list of array-like, default='auto' Categories (unique values) per feature: - 'auto' : Determine categories automatically from the training data. - list : categories[i] holds the categories expected in the ith column. The passed categories should not mix strings and numeric values, and should be sorted in case of numeric values. The used categories can be found in the categories_ attribute. dtype : number type, default np.float64 Desired dtype of output. handle_unknown : {'error', 'use_encoded_value'}, default='error' When set to 'error' an error will be raised in case an unknown categorical feature is present during transform. When set to 'use_encoded_value', the encoded value of unknown categories will be set to the value given for the parameter unknown_value . In :meth: inverse_transform , an unknown category will be denoted as None. .. versionadded:: 0.24 unknown_value : int or np.nan, default=None When the parameter handle_unknown is set to 'use_encoded_value', this parameter is required and will set the encoded value of unknown categories. It has to be distinct from the values used to encode any of the categories in fit . If set to np.nan, the dtype parameter must be a float dtype. .. versionadded:: 0.24 Attributes categories_ : list of arrays The categories of each feature determined during fit (in order of the features in X and corresponding with the output of transform ). This does not include categories that weren't seen during fit . See Also OneHotEncoder : Performs a one-hot encoding of categorical features. LabelEncoder : Encodes target labels with values between 0 and n_classes-1 . Examples Given a dataset with two features, we let the encoder find the unique values per feature and transform the data to an ordinal encoding. ``` >>> from sklearn.preprocessing import OrdinalEncoder >>> enc = OrdinalEncoder() >>> X = [['Male', 1], ['Female', 3], ['Female', 2]] >>> enc.fit(X) OrdinalEncoder() >>> enc.categories_ [array(['Female', 'Male'], dtype=object), array([1, 2, 3], dtype=object)] >>> enc.transform([['Female', 3], ['Male', 1]]) array([[0., 2.], [1., 0.]]) >>> enc.inverse_transform([[1, 0], [0, 1]]) array([['Male', 1], ['Female', 2]], dtype=object) ```","title":"OrdinalEncoder"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#methods_1","text":"fit(X, y=None) Fit the OrdinalEncoder to X. Parameters X : array-like of shape (n_samples, n_features) The data to determine the categories of each feature. y : None Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. inverse_transform(X) Convert the data back to the original representation. Parameters X : {array-like, sparse matrix} of shape (n_samples, n_features) The transformed data. Returns X_tr : ndarray of shape (n_samples, n_features) Inverse transformed array. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Transform X to ordinal codes. Parameters X : array-like of shape (n_samples, n_features) The data to encode. Returns X_out : ndarray of shape (n_samples, n_features) Transformed input.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#properties_1","text":"","title":"Properties"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#baseestimator","text":"BaseEstimator() Base class for all estimators in scikit-learn. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"BaseEstimator"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#methods_2","text":"get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#properties_2","text":"","title":"Properties"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#condorordinalencoder","text":"CondorOrdinalEncoder(nclasses=0, dtype= , kwargs) Base class for all estimators in scikit-learn. Notes All estimators should specify all the parameters that can be set at the class level in their __init__ as explicit keyword arguments (no *args or **kwargs ).","title":"CondorOrdinalEncoder"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#methods_3","text":"fit(X, y=None) Fit the CondorOrdinalEncoder to X. Parameters X : array-like of shape (n_samples, n_features) The data to determine the categories of each feature. y : None Ignored. This parameter exists only for compatibility with :class: ~sklearn.pipeline.Pipeline . Returns self fit_transform(X, y=None, fit_params) Fit to data, then transform it. Fits transformer to `X` and `y` with optional parameters `fit_params` and returns a transformed version of `X`. Parameters X : array-like of shape (n_samples, n_features) Input samples. y : array-like of shape (n_samples,) or (n_samples, n_outputs), default=None Target values (None for unsupervised transformations). **fit_params : dict Additional fit parameters. Returns X_new : ndarray array of shape (n_samples, n_features_new) Transformed array. get_params(deep=True) Get parameters for this estimator. Parameters deep : bool, default=True If True, will return the parameters for this estimator and contained subobjects that are estimators. Returns params : dict Parameter names mapped to their values. set_params( params) Set the parameters of this estimator. The method works on simple estimators as well as on nested objects (such as :class:`~sklearn.pipeline.Pipeline`). The latter have parameters of the form ``<component>__<parameter>`` so that it's possible to update each component of a nested object. Parameters **params : dict Estimator parameters. Returns self : estimator instance Estimator instance. transform(X) Transform X to ordinal arrays. Parameters X : array-like of shape (n_samples, 1) The labels data to encode. Returns X_out : ndarray of shape (n_samples, n_classes-1) Transformed input.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.labelencoder/#properties_3","text":"","title":"Properties"},{"location":"api_subpackages/condor_tensorflow.loss/","text":"condor_tensorflow version: 0.1.0-dev SparseCondorOrdinalCrossEntropy SparseCondorOrdinalCrossEntropy(importance_weights=None, from_type='ordinal_logits', name='ordinal_crossent', kwargs) Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) ``` When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom_training) for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: ```python with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)) ``` Methods call(y_true, y_pred) Invokes the Loss instance. Args: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` Returns: Loss values with the shape `[batch_size, d0, .. dN-1]`. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the config dictionary for a Loss instance. label_to_levels(label) None ordinal_loss(logits, levels, importance) Cross-entropy loss function designed for ordinal outcomes. Parameters logits: tf.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. levels: tf.Tensor, shape=(num_samples, num_classes-1) Encoded lables provided by CondorOrdinalEncoder. importance_weights: tf or np array of floats, shape(numclasses-1,) Importance weights for each binary classification task. Returns loss: tf.Tensor, shape=(num_samples,) Loss vector, note that tensorflow will reduce it to a single number automatically. Properties SparseOrdinalEarthMoversDistance SparseOrdinalEarthMoversDistance( kwargs) Computes earth movers distance for ordinal labels. Methods call(y_true, y_pred) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: Sparse Labels with values in {0,1,...,num_classes-1} from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the serializable config of the metric. Properties CondorOrdinalCrossEntropy CondorOrdinalCrossEntropy(importance_weights=None, from_type='ordinal_logits', name='ordinal_crossent', kwargs) Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) ``` When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom_training) for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: ```python with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)) ``` Methods call(y_true, y_pred) Invokes the Loss instance. Args: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` Returns: Loss values with the shape `[batch_size, d0, .. dN-1]`. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the config dictionary for a Loss instance. ordinal_loss(logits, levels, importance) Cross-entropy loss function designed for ordinal outcomes. Parameters logits: tf.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. levels: tf.Tensor, shape=(num_samples, num_classes-1) Encoded lables provided by CondorOrdinalEncoder. importance_weights: tf or np array of floats, shape(numclasses-1,) Importance weights for each binary classification task. Returns loss: tf.Tensor, shape=(num_samples,) Loss vector, note that tensorflow will reduce it to a single number automatically. Properties OrdinalEarthMoversDistance OrdinalEarthMoversDistance(name='earth_movers_distance', kwargs) Computes earth movers distance for ordinal labels. Methods call(y_true, y_pred) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the serializable config of the metric. Properties","title":"condor_tensorflow.loss"},{"location":"api_subpackages/condor_tensorflow.loss/#sparsecondorordinalcrossentropy","text":"SparseCondorOrdinalCrossEntropy(importance_weights=None, from_type='ordinal_logits', name='ordinal_crossent', kwargs) Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) ``` When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom_training) for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: ```python with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)) ```","title":"SparseCondorOrdinalCrossEntropy"},{"location":"api_subpackages/condor_tensorflow.loss/#methods","text":"call(y_true, y_pred) Invokes the Loss instance. Args: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` Returns: Loss values with the shape `[batch_size, d0, .. dN-1]`. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the config dictionary for a Loss instance. label_to_levels(label) None ordinal_loss(logits, levels, importance) Cross-entropy loss function designed for ordinal outcomes. Parameters logits: tf.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. levels: tf.Tensor, shape=(num_samples, num_classes-1) Encoded lables provided by CondorOrdinalEncoder. importance_weights: tf or np array of floats, shape(numclasses-1,) Importance weights for each binary classification task. Returns loss: tf.Tensor, shape=(num_samples,) Loss vector, note that tensorflow will reduce it to a single number automatically.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.loss/#properties","text":"","title":"Properties"},{"location":"api_subpackages/condor_tensorflow.loss/#sparseordinalearthmoversdistance","text":"SparseOrdinalEarthMoversDistance( kwargs) Computes earth movers distance for ordinal labels.","title":"SparseOrdinalEarthMoversDistance"},{"location":"api_subpackages/condor_tensorflow.loss/#methods_1","text":"call(y_true, y_pred) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: Sparse Labels with values in {0,1,...,num_classes-1} from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the serializable config of the metric.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.loss/#properties_1","text":"","title":"Properties"},{"location":"api_subpackages/condor_tensorflow.loss/#condorordinalcrossentropy","text":"CondorOrdinalCrossEntropy(importance_weights=None, from_type='ordinal_logits', name='ordinal_crossent', kwargs) Loss base class. To be implemented by subclasses: * `call()`: Contains the logic for loss calculation using `y_true`, `y_pred`. Example subclass implementation: ```python class MeanSquaredError(Loss): def call(self, y_true, y_pred): y_pred = tf.convert_to_tensor_v2(y_pred) y_true = tf.cast(y_true, y_pred.dtype) return tf.reduce_mean(math_ops.square(y_pred - y_true), axis=-1) ``` When used with `tf.distribute.Strategy`, outside of built-in training loops such as `tf.keras` `compile` and `fit`, please use 'SUM' or 'NONE' reduction types, and reduce losses explicitly in your training loop. Using 'AUTO' or 'SUM_OVER_BATCH_SIZE' will raise an error. Please see this custom training [tutorial]( https://www.tensorflow.org/tutorials/distribute/custom_training) for more details on this. You can implement 'SUM_OVER_BATCH_SIZE' using global batch size like: ```python with strategy.scope(): loss_obj = tf.keras.losses.CategoricalCrossentropy( reduction=tf.keras.losses.Reduction.NONE) .... loss = (tf.reduce_sum(loss_obj(labels, predictions)) * (1. / global_batch_size)) ```","title":"CondorOrdinalCrossEntropy"},{"location":"api_subpackages/condor_tensorflow.loss/#methods_2","text":"call(y_true, y_pred) Invokes the Loss instance. Args: y_true: Ground truth values. shape = `[batch_size, d0, .. dN]`, except sparse loss functions such as sparse categorical crossentropy where shape = `[batch_size, d0, .. dN-1]` y_pred: The predicted values. shape = `[batch_size, d0, .. dN]` Returns: Loss values with the shape `[batch_size, d0, .. dN-1]`. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the config dictionary for a Loss instance. ordinal_loss(logits, levels, importance) Cross-entropy loss function designed for ordinal outcomes. Parameters logits: tf.Tensor, shape=(num_samples,num_classes-1) Logit output of the final Dense(num_classes-1) layer. levels: tf.Tensor, shape=(num_samples, num_classes-1) Encoded lables provided by CondorOrdinalEncoder. importance_weights: tf or np array of floats, shape(numclasses-1,) Importance weights for each binary classification task. Returns loss: tf.Tensor, shape=(num_samples,) Loss vector, note that tensorflow will reduce it to a single number automatically.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.loss/#properties_2","text":"","title":"Properties"},{"location":"api_subpackages/condor_tensorflow.loss/#ordinalearthmoversdistance","text":"OrdinalEarthMoversDistance(name='earth_movers_distance', kwargs) Computes earth movers distance for ordinal labels.","title":"OrdinalEarthMoversDistance"},{"location":"api_subpackages/condor_tensorflow.loss/#methods_3","text":"call(y_true, y_pred) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. from_config(config) Instantiates a Loss from its config (output of get_config() ). Args: config: Output of `get_config()`. Returns: A `Loss` instance. get_config() Returns the serializable config of the metric.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.loss/#properties_3","text":"","title":"Properties"},{"location":"api_subpackages/condor_tensorflow.metrics/","text":"condor_tensorflow version: 0.1.0-dev OrdinalMeanAbsoluteError OrdinalMeanAbsoluteError( args, * kwargs) Computes mean absolute error for ordinal labels. Methods add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. SparseOrdinalMeanAbsoluteError SparseOrdinalMeanAbsoluteError( args, * kwargs) Computes mean absolute error for ordinal labels. Methods add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. OrdinalAccuracy OrdinalAccuracy( args, * kwargs) Computes accuracy for ordinal labels (tolerance is allowed rank distance to be considered 'correct' predictions). Methods add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes accuracy for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. SparseOrdinalAccuracy SparseOrdinalAccuracy( args, * kwargs) Computes accuracy for ordinal labels (tolerance is allowed rank distance to be considered 'correct' predictions). Methods add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes accuracy for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"condor_tensorflow.metrics"},{"location":"api_subpackages/condor_tensorflow.metrics/#ordinalmeanabsoluteerror","text":"OrdinalMeanAbsoluteError( args, * kwargs) Computes mean absolute error for ordinal labels.","title":"OrdinalMeanAbsoluteError"},{"location":"api_subpackages/condor_tensorflow.metrics/#methods","text":"add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.metrics/#sparseordinalmeanabsoluteerror","text":"SparseOrdinalMeanAbsoluteError( args, * kwargs) Computes mean absolute error for ordinal labels.","title":"SparseOrdinalMeanAbsoluteError"},{"location":"api_subpackages/condor_tensorflow.metrics/#methods_1","text":"add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes mean absolute error for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.metrics/#ordinalaccuracy","text":"OrdinalAccuracy( args, * kwargs) Computes accuracy for ordinal labels (tolerance is allowed rank distance to be considered 'correct' predictions).","title":"OrdinalAccuracy"},{"location":"api_subpackages/condor_tensorflow.metrics/#methods_2","text":"add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes accuracy for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"Methods"},{"location":"api_subpackages/condor_tensorflow.metrics/#sparseordinalaccuracy","text":"SparseOrdinalAccuracy( args, * kwargs) Computes accuracy for ordinal labels (tolerance is allowed rank distance to be considered 'correct' predictions).","title":"SparseOrdinalAccuracy"},{"location":"api_subpackages/condor_tensorflow.metrics/#methods_3","text":"add_loss(losses, kwargs) Add loss tensor(s), potentially dependent on layer inputs. Some losses (for instance, activity regularization losses) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.losses` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This method can be used inside a subclassed layer or model's `call` function, in which case `losses` should be a Tensor or list of Tensors. Example: ```python class MyLayer(tf.keras.layers.Layer): def call(self, inputs): self.add_loss(tf.abs(tf.reduce_mean(inputs))) return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any loss Tensors passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These losses become part of the model's topology and are tracked in `get_config`. Example: ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Activity regularization. model.add_loss(tf.abs(tf.reduce_mean(x))) ``` If this is not the case for your loss (if, for example, your loss references a `Variable` of one of the model's layers), you can wrap your loss in a zero-argument lambda. These losses are not tracked as part of the model's topology since they can't be serialized. Example: ```python inputs = tf.keras.Input(shape=(10,)) d = tf.keras.layers.Dense(10) x = d(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) # Weight regularization. model.add_loss(lambda: tf.reduce_mean(d.kernel)) ``` Arguments: losses: Loss tensor, or list/tuple of tensors. Rather than tensors, losses may also be zero-argument callables which create a loss tensor. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: inputs - Deprecated, will be automatically inferred. add_metric(value, name=None, kwargs) Adds metric tensor to the layer. This method can be used inside the `call()` method of a subclassed layer or model. ```python class MyMetricLayer(tf.keras.layers.Layer): def __init__(self): super(MyMetricLayer, self).__init__(name='my_metric_layer') self.mean = tf.keras.metrics.Mean(name='metric_1') def call(self, inputs): self.add_metric(self.mean(x)) self.add_metric(tf.reduce_sum(x), name='metric_2') return inputs ``` This method can also be called directly on a Functional Model during construction. In this case, any tensor passed to this Model must be symbolic and be able to be traced back to the model's `Input`s. These metrics become part of the model's topology and are tracked when you save the model via `save()`. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(math_ops.reduce_sum(x), name='metric_1') ``` Note: Calling `add_metric()` with the result of a metric object on a Functional Model, as shown in the example below, is not supported. This is because we cannot trace the metric result tensor back to the model's inputs. ```python inputs = tf.keras.Input(shape=(10,)) x = tf.keras.layers.Dense(10)(inputs) outputs = tf.keras.layers.Dense(1)(x) model = tf.keras.Model(inputs, outputs) model.add_metric(tf.keras.metrics.Mean()(x), name='metric_1') ``` Args: value: Metric tensor. name: String metric name. **kwargs: Additional keyword arguments for backward compatibility. Accepted values: aggregation - When the value tensor provided is not the result of calling a keras.Metric instance, it will be aggregated by default using a keras.Metric.Mean . add_update(updates, inputs=None) Add update op(s), potentially dependent on layer inputs. Weight updates (for instance, the updates of the moving mean and variance in a BatchNormalization layer) may be dependent on the inputs passed when calling a layer. Hence, when reusing the same layer on different inputs `a` and `b`, some entries in `layer.updates` may be dependent on `a` and some on `b`. This method automatically keeps track of dependencies. This call is ignored when eager execution is enabled (in that case, variable updates are run on the fly and thus do not need to be tracked for later execution). Arguments: updates: Update op, or list/tuple of update ops, or zero-arg callable that returns an update op. A zero-arg callable should be passed in order to disable running the updates by setting `trainable=False` on this Layer, when executing in Eager mode. inputs: Deprecated, will be automatically inferred. add_variable( args, * kwargs) Deprecated, do NOT use! Alias for add_weight . add_weight(name, shape=(), aggregation= , synchronization= , initializer=None, dtype=None) Adds state variable. Only for use by subclasses. apply(inputs, args, * kwargs) Deprecated, do NOT use! This is an alias of `self.__call__`. Arguments: inputs: Input tensor(s). *args: additional positional arguments to be passed to `self.call`. **kwargs: additional keyword arguments to be passed to self.call . Returns: Output tensor(s). build(input_shape) Creates the variables of the layer (optional, for subclass implementers). This is a method that implementers of subclasses of `Layer` or `Model` can override if they need a state-creation step in-between layer instantiation and layer call. This is typically used to create the weights of `Layer` subclasses. Arguments: input_shape: Instance of `TensorShape`, or list of instances of `TensorShape` if the layer expects a list of inputs (one instance per input). call(inputs, kwargs) This is where the layer's logic lives. Note here that `call()` method in `tf.keras` is little bit different from `keras` API. In `keras` API, you can pass support masking for layers as additional arguments. Whereas `tf.keras` has `compute_mask()` method to support masking. Arguments: inputs: Input tensor, or list/tuple of input tensors. **kwargs: Additional keyword arguments. Currently unused. Returns: A tensor or list/tuple of tensors. compute_mask(inputs, mask=None) Computes an output mask tensor. Arguments: inputs: Tensor or list of tensors. mask: Tensor or list of tensors. Returns: None or a tensor (or list of tensors, one per output tensor of the layer). compute_output_shape(input_shape) Computes the output shape of the layer. If the layer has not been built, this method will call `build` on the layer. This assumes that the layer will later be used with inputs that match the input shape provided here. Arguments: input_shape: Shape tuple (tuple of integers) or list of shape tuples (one per output tensor of the layer). Shape tuples can include None for free dimensions, instead of an integer. Returns: An input shape tuple. compute_output_signature(input_signature) Compute the output tensor signature of the layer based on the inputs. Unlike a TensorShape object, a TensorSpec object contains both shape and dtype information for a tensor. This method allows layers to provide output dtype information if it is different from the input dtype. For any layer that doesn't implement this function, the framework will fall back to use `compute_output_shape`, and will assume that the output dtype matches the input dtype. Args: input_signature: Single TensorSpec or nested structure of TensorSpec objects, describing a candidate input for the layer. Returns: Single TensorSpec or nested structure of TensorSpec objects, describing how the layer would transform the provided input. Raises: TypeError: If input_signature contains a non-TensorSpec object. count_params() Count the total number of scalars composing the weights. Returns: An integer count. Raises: ValueError: if the layer isn't yet built (in which case its weights aren't yet defined). from_config(config) Creates a layer from its config. This method is the reverse of `get_config`, capable of instantiating the same layer from the config dictionary. It does not handle layer connectivity (handled by Network), nor weights (handled by `set_weights`). Arguments: config: A Python dictionary, typically the output of get_config. Returns: A layer instance. get_config() Returns the serializable config of the metric. get_input_at(node_index) Retrieves the input tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_input_mask_at(node_index) Retrieves the input mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple inputs). get_input_shape_at(node_index) Retrieves the input shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple inputs). Raises: RuntimeError: If called in Eager mode. get_losses_for(inputs) Deprecated, do NOT use! Retrieves losses relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of loss tensors of the layer that depend on `inputs`. get_output_at(node_index) Retrieves the output tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A tensor (or list of tensors if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_output_mask_at(node_index) Retrieves the output mask tensor(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A mask tensor (or list of tensors if the layer has multiple outputs). get_output_shape_at(node_index) Retrieves the output shape(s) of a layer at a given node. Arguments: node_index: Integer, index of the node from which to retrieve the attribute. E.g. `node_index=0` will correspond to the first time the layer was called. Returns: A shape tuple (or list of shape tuples if the layer has multiple outputs). Raises: RuntimeError: If called in Eager mode. get_updates_for(inputs) Deprecated, do NOT use! Retrieves updates relevant to a specific set of inputs. Arguments: inputs: Input tensor or list/tuple of input tensors. Returns: List of update ops of the layer that depend on `inputs`. get_weights() Returns the current weights of the layer. The weights of a layer represent the state of the layer. This function returns both trainable and non-trainable weight values associated with this layer as a list of Numpy arrays, which can in turn be used to load state into similarly parameterized layers. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Returns: Weights values as a list of numpy arrays. <hr> *reset_state()* Resets all of the metric state variables at the start of each epoch. <hr> *reset_states()* Resets all of the metric state variables. This function is called between epochs/steps, when a metric is evaluated during training. <hr> *result()* Computes and returns the metric value tensor. Result computation is an idempotent operation that simply calculates the metric value using the state variables. <hr> *set_weights(weights)* Sets the weights of the layer, from Numpy arrays. The weights of a layer represent the state of the layer. This function sets the weight values from numpy arrays. The weight values should be passed in the order they are created by the layer. Note that the layer's weights must be instantiated before calling this function by calling the layer. For example, a Dense layer returns a list of two values-- per-output weights and the bias value. These can be used to set the weights of another Dense layer: ``` >>> a = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(1.)) >>> a_out = a(tf.convert_to_tensor([[1., 2., 3.]])) >>> a.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] >>> b = tf.keras.layers.Dense(1, ... kernel_initializer=tf.constant_initializer(2.)) >>> b_out = b(tf.convert_to_tensor([[10., 20., 30.]])) >>> b.get_weights() [array([[2.], [2.], [2.]], dtype=float32), array([0.], dtype=float32)] >>> b.set_weights(a.get_weights()) >>> b.get_weights() [array([[1.], [1.], [1.]], dtype=float32), array([0.], dtype=float32)] Arguments: weights: a list of Numpy arrays. The number of arrays and their shape must match number of the dimensions of the weights of the layer (i.e. it should match the output of `get_weights`). Raises: ValueError: If the provided weights list does not match the layer's specifications. update_state(y_true, y_pred, sample_weight=None) Computes accuracy for ordinal labels. Args: y_true: Cumulatiuve logits from CondorOrdinal layer. y_pred: CondorOrdinal Encoded Labels. sample_weight (optional): Not implemented. with_name_scope(method) Decorator to automatically enter the module name scope. ``` >>> class MyModule(tf.Module): ... @tf.Module.with_name_scope ... def __call__(self, x): ... if not hasattr(self, 'w'): ... self.w = tf.Variable(tf.random.normal([x.shape[1], 3])) ... return tf.matmul(x, self.w) Using the above module would produce `tf.Variable`s and `tf.Tensor`s whose names included the module name: >>> mod = MyModule() >>> mod(tf.ones([1, 2])) <tf.Tensor: shape=(1, 3), dtype=float32, numpy=..., dtype=float32)> >>> mod.w <tf.Variable 'my_module/Variable:0' shape=(2, 3) dtype=float32, numpy=..., dtype=float32)> Args: method: The method to wrap. Returns: The original method wrapped such that it enters the module's name scope. ### Properties <hr> *activity_regularizer* Optional regularizer function for the output of this layer. <hr> *compute_dtype* The dtype of the layer's computations. This is equivalent to `Layer.dtype_policy.compute_dtype`. Unless mixed precision is used, this is the same as `Layer.dtype`, the dtype of the weights. Layers automatically cast their inputs to the compute dtype, which causes computations and the output to be in the compute dtype as well. This is done by the base Layer class in `Layer.__call__`, so you do not have to insert these casts if implementing your own layer. Layers often perform certain internal computations in higher precision when `compute_dtype` is float16 or bfloat16 for numeric stability. The output will still typically be float16 or bfloat16 in such cases. Returns: The layer's compute dtype. <hr> *dtype* The dtype of the layer weights. This is equivalent to `Layer.dtype_policy.variable_dtype`. Unless mixed precision is used, this is the same as `Layer.compute_dtype`, the dtype of the layer's computations. <hr> *dtype_policy* The dtype policy associated with this layer. This is an instance of a `tf.keras.mixed_precision.Policy`. <hr> *dynamic* Whether the layer is dynamic (eager-only); set in the constructor. <hr> *inbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *input* Retrieves the input tensor(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer. Returns: Input tensor or list of input tensors. Raises: RuntimeError: If called in Eager mode. AttributeError: If no inbound nodes are found. <hr> *input_mask* Retrieves the input mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Input mask tensor (potentially None) or list of input mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *input_shape* Retrieves the input shape(s) of a layer. Only applicable if the layer has exactly one input, i.e. if it is connected to one incoming layer, or if all inputs have the same shape. Returns: Input shape, as an integer shape tuple (or list of shape tuples, one tuple per input tensor). Raises: AttributeError: if the layer has no defined input_shape. RuntimeError: if called in Eager mode. <hr> *input_spec* `InputSpec` instance(s) describing the input format for this layer. When you create a layer subclass, you can set `self.input_spec` to enable the layer to run input compatibility checks when it is called. Consider a `Conv2D` layer: it can only be called on a single input tensor of rank 4. As such, you can set, in `__init__()`: ```python self.input_spec = tf.keras.layers.InputSpec(ndim=4) ``` Now, if you try to call the layer on an input that isn't rank 4 (for instance, an input of shape `(2,)`, it will raise a nicely-formatted error: ``` ValueError: Input 0 of layer conv2d is incompatible with the layer: expected ndim=4, found ndim=1. Full shape received: [2] ``` Input checks that can be specified via `input_spec` include: - Structure (e.g. a single input, a list of 2 inputs, etc) - Shape - Rank (ndim) - Dtype For more information, see `tf.keras.layers.InputSpec`. Returns: A `tf.keras.layers.InputSpec` instance, or nested structure thereof. <hr> *losses* List of losses added using the `add_loss()` API. Variable regularization tensors are created when this property is accessed, so it is eager safe: accessing `losses` under a `tf.GradientTape` will propagate gradients back to the corresponding variables. Examples: ``` >>> class MyLayer(tf.keras.layers.Layer): ... def call(self, inputs): ... self.add_loss(tf.abs(tf.reduce_mean(inputs))) ... return inputs >>> l = MyLayer() >>> l(np.ones((10, 1))) >>> l.losses [1.0] >>> inputs = tf.keras.Input(shape=(10,)) >>> x = tf.keras.layers.Dense(10)(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Activity regularization. >>> len(model.losses) 0 >>> model.add_loss(tf.abs(tf.reduce_mean(x))) >>> len(model.losses) 1 >>> inputs = tf.keras.Input(shape=(10,)) >>> d = tf.keras.layers.Dense(10, kernel_initializer='ones') >>> x = d(inputs) >>> outputs = tf.keras.layers.Dense(1)(x) >>> model = tf.keras.Model(inputs, outputs) >>> # Weight regularization. >>> model.add_loss(lambda: tf.reduce_mean(d.kernel)) >>> model.losses [<tf.Tensor: shape=(), dtype=float32, numpy=1.0>] Returns: A list of tensors. metrics List of metrics added using the add_metric() API. Example: ``` >>> input = tf.keras.layers.Input(shape=(3,)) >>> d = tf.keras.layers.Dense(2) >>> output = d(input) >>> d.add_metric(tf.reduce_max(output), name='max') >>> d.add_metric(tf.reduce_min(output), name='min') >>> [m.name for m in d.metrics] ['max', 'min'] Returns: A list of `Metric` objects. <hr> *name* Name of the layer (string), set in the constructor. <hr> *name_scope* Returns a `tf.name_scope` instance for this class. <hr> *non_trainable_variables* None <hr> *non_trainable_weights* List of all non-trainable weights tracked by this layer. Non-trainable weights are *not* updated during training. They are expected to be updated manually in `call()`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of non-trainable variables. <hr> *outbound_nodes* Deprecated, do NOT use! Only for compatibility with external Keras. <hr> *output* Retrieves the output tensor(s) of a layer. Only applicable if the layer has exactly one output, i.e. if it is connected to one incoming layer. Returns: Output tensor or list of output tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. RuntimeError: if called in Eager mode. <hr> *output_mask* Retrieves the output mask tensor(s) of a layer. Only applicable if the layer has exactly one inbound node, i.e. if it is connected to one incoming layer. Returns: Output mask tensor (potentially None) or list of output mask tensors. Raises: AttributeError: if the layer is connected to more than one incoming layers. <hr> *output_shape* Retrieves the output shape(s) of a layer. Only applicable if the layer has one output, or if all outputs have the same shape. Returns: Output shape, as an integer shape tuple (or list of shape tuples, one tuple per output tensor). Raises: AttributeError: if the layer has no defined output shape. RuntimeError: if called in Eager mode. <hr> *stateful* None <hr> *submodules* Sequence of all sub-modules. Submodules are modules which are properties of this module, or found as properties of modules which are properties of this module (and so on). ``` >>> a = tf.Module() >>> b = tf.Module() >>> c = tf.Module() >>> a.b = b >>> b.c = c >>> list(a.submodules) == [b, c] True >>> list(b.submodules) == [c] True >>> list(c.submodules) == [] True Returns: A sequence of all submodules. supports_masking Whether this layer supports computing a mask using compute_mask . trainable None trainable_variables Sequence of trainable variables owned by this module and its submodules. Note: this method uses reflection to find variables on the current instance and submodules. For performance reasons you may wish to cache the result of calling this method if you don't expect the return value to change. Returns: A sequence of variables for the current module (sorted by attribute name) followed by variables from all submodules recursively (breadth first). trainable_weights List of all trainable weights tracked by this layer. Trainable weights are updated via gradient descent during training. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of trainable variables. updates None variable_dtype Alias of Layer.dtype , the dtype of the weights. variables Returns the list of all layer variables/weights. Alias of `self.weights`. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables. weights Returns the list of all layer variables/weights. Note: This will not track the weights of nested `tf.Modules` that are not themselves Keras layers. Returns: A list of variables.","title":"Methods"},{"location":"tutorials/amazon/","text":"CONDOR Ordinal classification/regression in Tensorflow Keras Import statements import numpy as np import sklearn from sklearn import model_selection from sklearn.model_selection import train_test_split import pandas as pd from scipy import special import tensorflow_hub as hub import os import json import gzip from urllib.request import urlopen import tensorflow as tf print(\"Tensorflow version\", tf.__version__) import condor_tensorflow as condor print(\"CONDOR Ordinal version:\", condor.__version__) Amazon reviews and 5-star ratings Amazon review data via https://nijianmo.github.io/amazon/index.html#subsets !curl -o Prime_Pantry_5.json.gz http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Prime_Pantry_5.json.gz data = [] with gzip.open('Prime_Pantry_5.json.gz') as f: for l in f: data.append(json.loads(l.strip())) df = pd.DataFrame.from_dict(data) df = df[['overall', 'reviewText']] # There is a large amount of duplicate text in here, possibly due to paid/fraudulent reviews. df.drop_duplicates(\"reviewText\", inplace = True) # Some of the text is blank, which causes an obscure error about floating point conversion. df.dropna(inplace = True) print(len(df)) print(df.head()) outcome_col = \"overall\" text_col = \"reviewText\" # We subtract the minimum value from the outcomes so that they start at 0. df[outcome_col] = df[outcome_col].values - df[outcome_col].min() print(\"\\n\", df.overall.value_counts()) # TODO: define automatically based on the number of unique values in the outcome variable. num_classes = 5 # Train/Test split text_train, text_test, labels_train, labels_test = \\ train_test_split(df[text_col].values, df[outcome_col].values, test_size = 10000, random_state = 1) print(\"Training text shape:\", text_train.shape) print(\"Training labels shape:\", labels_train.shape) print(\"Testing text shape:\", text_test.shape) print(\"Testing labels shape:\", labels_test.shape) Universal Sentence Encoder model (minimal code changes) # This takes 20 - 30 seconds. # Clear our GPU memory to stay efficient. tf.keras.backend.clear_session() input_text = tf.keras.layers.Input(shape = [], dtype = tf.string, name = 'input_text') model_url = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\") base_model = hub.KerasLayer(model_url, input_shape = [], dtype = tf.string, trainable = False) embedded = base_model(input_text) x = tf.keras.layers.Dense(64, activation = 'relu')(embedded) x = tf.keras.layers.Dropout(0.1)(x) output =tf.keras.layers.Dense(num_classes-1)(x) model = tf.keras.Model(inputs = input_text, outputs = output) model.summary() model.compile(loss = condor.SparseCondorOrdinalCrossEntropy(), metrics = [condor.SparseOrdinalEarthMoversDistance(), condor.SparseOrdinalMeanAbsoluteError()], optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)) # Encode a test string and take a look at the first ten dimensions. base_model(np.array([\"test_string\"])).numpy()[0, :10] history = model.fit(x = text_train, y = labels_train, epochs = 5, batch_size = 32, validation_split = 0.2, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 2, min_delta = 0.001, restore_best_weights = True)]) Evaluate # For comparison, CORAL achieves loss 0.7962, MAE 0.3195 model.evaluate(text_test, labels_test) # Generate predictions - initially these are cumulative logits. preds = model.predict(text_test) print(preds) # Convert cumulative logits to probabilities for each class aka rank or label. probs = pd.DataFrame(condor.ordinal_softmax(preds).numpy()) print(probs.head(10)) print(labels_test[:10]) Evaluate accuracy # Evaluate accuracy and mean absolute error labels_v1 = probs.idxmax(axis = 1) print(\"Accuracy of label version 1:\", np.mean(labels_v1 == labels_test)) # Compare to logit-based cumulative probs cum_probs = pd.DataFrame(preds).apply(special.expit).cumprod(axis=1) # Calculate the labels using the style of Cao et al. labels_v2 = cum_probs.apply(lambda x: x > 0.5).sum(axis = 1) print(\"Accuracy of label version 2:\", np.mean(labels_v2 == labels_test)) Evaluate mean absolute label error This is effectively an ordinal version of 1 - accuracy. # These do not correspond with what we get from the model evaluation. Something must be off in one of these. print(\"Mean absolute label error version 1:\", np.mean(np.abs(labels_v1 - labels_test))) print(\"Mean absolute label error version 2:\", np.mean(np.abs(labels_v2 - labels_test))) print(\"Root mean squared label error version 1:\", np.sqrt(np.mean(np.square(labels_v1 - labels_test)))) print(\"Root mean squared label error version 2:\", np.sqrt(np.mean(np.square(labels_v2 - labels_test)))) # Review how absolute error is calculated for ordinal labels: pd.DataFrame({\"true\": labels_test, \"pred_v2\": labels_v1, \"abs\": labels_v2 - labels_test}).head() Universal Sentence Encoder model (speed up using encodings) The \"Sparse\" versions of the CONDOR API are convenient and require minimal code changes. However there is a performance overhead compared to if we pre-encode the labels using CONDORs ordinal encoder. The sparse API is basically encoding on the fly inside the training loop. Also as we will see later, the labels do not always come encoded as 0,1,...,K-1. In this case, using the CondorOrdinalEncoder will help transform labels into ordinal-ready values. # pre-encoding runs very fast so the savings later are worth it enc = condor.CondorOrdinalEncoder(nclasses=num_classes) enc_labs_train = enc.fit_transform(labels_train) enc_labs_test = enc.transform(labels_test) # Note the lack of \"Sparse\" in the condor functions here model.compile(loss = condor.CondorOrdinalCrossEntropy(), metrics = [condor.OrdinalEarthMoversDistance(), condor.OrdinalMeanAbsoluteError()], optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)) # note the encoded labels are passed to the fit now history = model.fit(x = text_train, y = enc_labs_train, epochs = 5, batch_size = 32, validation_split = 0.2, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 2, min_delta = 0.001, restore_best_weights = True)]) model.evaluate(text_test, enc_labs_test) More examples of label encoding capabilities Here we demo the features of the ordinal encoder. # Here the ordinal encoder figures out how many classes there are automatically # and orders them in the default sklearn OrdinalEncoder fashion # (i.e., alphabetically here) labels = np.array(['a','b','c','d','e']) enc_labs = condor.CondorOrdinalEncoder().fit_transform(labels) print(enc_labs) # Here the ordinal encoder figures out how many classes there are automatically # and orders them in the default sklearn OrdinalEncoder fashion # (i.e., alphabetically here). This time it is dealing with a basic list. labels = ['a','b','c','d','e'] enc_labs = condor.CondorOrdinalEncoder().fit_transform(labels) print(enc_labs) # Here we wish to specify the order to be different from alphabetical. Note # this would also allow \"missing\" categories to be included in proper order. labels = ['low','med','high'] enc = condor.CondorOrdinalEncoder(categories=[['low', 'med', 'high']]) enc_labs = enc.fit_transform(labels) print(enc_labs)","title":"Amazon Reveiews"},{"location":"tutorials/amazon/#condor-ordinal-classificationregression-in-tensorflow-keras","text":"","title":"CONDOR Ordinal classification/regression in Tensorflow Keras"},{"location":"tutorials/amazon/#import-statements","text":"import numpy as np import sklearn from sklearn import model_selection from sklearn.model_selection import train_test_split import pandas as pd from scipy import special import tensorflow_hub as hub import os import json import gzip from urllib.request import urlopen import tensorflow as tf print(\"Tensorflow version\", tf.__version__) import condor_tensorflow as condor print(\"CONDOR Ordinal version:\", condor.__version__)","title":"Import statements"},{"location":"tutorials/amazon/#amazon-reviews-and-5-star-ratings","text":"Amazon review data via https://nijianmo.github.io/amazon/index.html#subsets !curl -o Prime_Pantry_5.json.gz http://deepyeti.ucsd.edu/jianmo/amazon/categoryFilesSmall/Prime_Pantry_5.json.gz data = [] with gzip.open('Prime_Pantry_5.json.gz') as f: for l in f: data.append(json.loads(l.strip())) df = pd.DataFrame.from_dict(data) df = df[['overall', 'reviewText']] # There is a large amount of duplicate text in here, possibly due to paid/fraudulent reviews. df.drop_duplicates(\"reviewText\", inplace = True) # Some of the text is blank, which causes an obscure error about floating point conversion. df.dropna(inplace = True) print(len(df)) print(df.head()) outcome_col = \"overall\" text_col = \"reviewText\" # We subtract the minimum value from the outcomes so that they start at 0. df[outcome_col] = df[outcome_col].values - df[outcome_col].min() print(\"\\n\", df.overall.value_counts()) # TODO: define automatically based on the number of unique values in the outcome variable. num_classes = 5 # Train/Test split text_train, text_test, labels_train, labels_test = \\ train_test_split(df[text_col].values, df[outcome_col].values, test_size = 10000, random_state = 1) print(\"Training text shape:\", text_train.shape) print(\"Training labels shape:\", labels_train.shape) print(\"Testing text shape:\", text_test.shape) print(\"Testing labels shape:\", labels_test.shape)","title":"Amazon reviews and 5-star ratings"},{"location":"tutorials/amazon/#universal-sentence-encoder-model-minimal-code-changes","text":"# This takes 20 - 30 seconds. # Clear our GPU memory to stay efficient. tf.keras.backend.clear_session() input_text = tf.keras.layers.Input(shape = [], dtype = tf.string, name = 'input_text') model_url = hub.load(\"https://tfhub.dev/google/universal-sentence-encoder-large/5\") base_model = hub.KerasLayer(model_url, input_shape = [], dtype = tf.string, trainable = False) embedded = base_model(input_text) x = tf.keras.layers.Dense(64, activation = 'relu')(embedded) x = tf.keras.layers.Dropout(0.1)(x) output =tf.keras.layers.Dense(num_classes-1)(x) model = tf.keras.Model(inputs = input_text, outputs = output) model.summary() model.compile(loss = condor.SparseCondorOrdinalCrossEntropy(), metrics = [condor.SparseOrdinalEarthMoversDistance(), condor.SparseOrdinalMeanAbsoluteError()], optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)) # Encode a test string and take a look at the first ten dimensions. base_model(np.array([\"test_string\"])).numpy()[0, :10] history = model.fit(x = text_train, y = labels_train, epochs = 5, batch_size = 32, validation_split = 0.2, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 2, min_delta = 0.001, restore_best_weights = True)])","title":"Universal Sentence Encoder model (minimal code changes)"},{"location":"tutorials/amazon/#evaluate","text":"# For comparison, CORAL achieves loss 0.7962, MAE 0.3195 model.evaluate(text_test, labels_test) # Generate predictions - initially these are cumulative logits. preds = model.predict(text_test) print(preds) # Convert cumulative logits to probabilities for each class aka rank or label. probs = pd.DataFrame(condor.ordinal_softmax(preds).numpy()) print(probs.head(10)) print(labels_test[:10])","title":"Evaluate"},{"location":"tutorials/amazon/#evaluate-accuracy","text":"# Evaluate accuracy and mean absolute error labels_v1 = probs.idxmax(axis = 1) print(\"Accuracy of label version 1:\", np.mean(labels_v1 == labels_test)) # Compare to logit-based cumulative probs cum_probs = pd.DataFrame(preds).apply(special.expit).cumprod(axis=1) # Calculate the labels using the style of Cao et al. labels_v2 = cum_probs.apply(lambda x: x > 0.5).sum(axis = 1) print(\"Accuracy of label version 2:\", np.mean(labels_v2 == labels_test))","title":"Evaluate accuracy"},{"location":"tutorials/amazon/#evaluate-mean-absolute-label-error","text":"This is effectively an ordinal version of 1 - accuracy. # These do not correspond with what we get from the model evaluation. Something must be off in one of these. print(\"Mean absolute label error version 1:\", np.mean(np.abs(labels_v1 - labels_test))) print(\"Mean absolute label error version 2:\", np.mean(np.abs(labels_v2 - labels_test))) print(\"Root mean squared label error version 1:\", np.sqrt(np.mean(np.square(labels_v1 - labels_test)))) print(\"Root mean squared label error version 2:\", np.sqrt(np.mean(np.square(labels_v2 - labels_test)))) # Review how absolute error is calculated for ordinal labels: pd.DataFrame({\"true\": labels_test, \"pred_v2\": labels_v1, \"abs\": labels_v2 - labels_test}).head()","title":"Evaluate mean absolute label error"},{"location":"tutorials/amazon/#universal-sentence-encoder-model-speed-up-using-encodings","text":"The \"Sparse\" versions of the CONDOR API are convenient and require minimal code changes. However there is a performance overhead compared to if we pre-encode the labels using CONDORs ordinal encoder. The sparse API is basically encoding on the fly inside the training loop. Also as we will see later, the labels do not always come encoded as 0,1,...,K-1. In this case, using the CondorOrdinalEncoder will help transform labels into ordinal-ready values. # pre-encoding runs very fast so the savings later are worth it enc = condor.CondorOrdinalEncoder(nclasses=num_classes) enc_labs_train = enc.fit_transform(labels_train) enc_labs_test = enc.transform(labels_test) # Note the lack of \"Sparse\" in the condor functions here model.compile(loss = condor.CondorOrdinalCrossEntropy(), metrics = [condor.OrdinalEarthMoversDistance(), condor.OrdinalMeanAbsoluteError()], optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001)) # note the encoded labels are passed to the fit now history = model.fit(x = text_train, y = enc_labs_train, epochs = 5, batch_size = 32, validation_split = 0.2, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 2, min_delta = 0.001, restore_best_weights = True)]) model.evaluate(text_test, enc_labs_test)","title":"Universal Sentence Encoder model (speed up using encodings)"},{"location":"tutorials/amazon/#more-examples-of-label-encoding-capabilities","text":"Here we demo the features of the ordinal encoder. # Here the ordinal encoder figures out how many classes there are automatically # and orders them in the default sklearn OrdinalEncoder fashion # (i.e., alphabetically here) labels = np.array(['a','b','c','d','e']) enc_labs = condor.CondorOrdinalEncoder().fit_transform(labels) print(enc_labs) # Here the ordinal encoder figures out how many classes there are automatically # and orders them in the default sklearn OrdinalEncoder fashion # (i.e., alphabetically here). This time it is dealing with a basic list. labels = ['a','b','c','d','e'] enc_labs = condor.CondorOrdinalEncoder().fit_transform(labels) print(enc_labs) # Here we wish to specify the order to be different from alphabetical. Note # this would also allow \"missing\" categories to be included in proper order. labels = ['low','med','high'] enc = condor.CondorOrdinalEncoder(categories=[['low', 'med', 'high']]) enc_labs = enc.fit_transform(labels) print(enc_labs)","title":"More examples of label encoding capabilities"},{"location":"tutorials/mnist/","text":"CONDOR Ordinal classification/regression in Tensorflow Keras Import statements import numpy as np import sklearn from sklearn import model_selection from sklearn.model_selection import train_test_split import pandas as pd from scipy import special import tensorflow_hub as hub import os import json import gzip from urllib.request import urlopen import tensorflow as tf print(\"Tensorflow version\", tf.__version__) import condor_tensorflow as condor print(\"CONDOR Ordinal version:\", condor.__version__) MNIST toy example This outcome is not actually ordinal, it's categorical. We're just using it as a toy example to show how the different components are used. # Hyperparameters random_seed = 1 # Not yet used learning_rate = 0.05 batch_size = 128 num_epochs = 2 # Architecture NUM_CLASSES = 10 # Fetch and format the mnist data (mnist_images, mnist_labels), (mnist_images_test, mnist_labels_test) = tf.keras.datasets.mnist.load_data() # Split off a validation dataset for early stopping mnist_images, mnist_images_val, mnist_labels, mnist_labels_val = \\ model_selection.train_test_split(mnist_images, mnist_labels, test_size = 5000, random_state = 1) print(\"Shape of training images:\", mnist_images.shape) print(\"Shape of training labels:\", mnist_labels.shape) print(\"Shape of test images:\", mnist_images_test.shape) print(\"Shape of test labels:\", mnist_labels_test.shape) print(\"Shape of validation images:\", mnist_images_val.shape) print(\"Shape of validation labels:\", mnist_labels_val.shape) # Also rescales to 0-1 range. dataset = tf.data.Dataset.from_tensor_slices( (tf.cast(mnist_images[..., tf.newaxis] / 255, tf.float32), tf.cast(mnist_labels, tf.int64))) dataset = dataset.shuffle(1000).batch(batch_size) test_dataset = tf.data.Dataset.from_tensor_slices( (tf.cast(mnist_images_test[..., tf.newaxis] / 255, tf.float32), tf.cast(mnist_labels_test, tf.int64))) #test_dataset = test_dataset.shuffle(1000).batch(batch_size) # Here we do not shuffle the test dataset. test_dataset = test_dataset.batch(batch_size) val_dataset = tf.data.Dataset.from_tensor_slices( (tf.cast(mnist_images_val[..., tf.newaxis] / 255, tf.float32), tf.cast(mnist_labels_val, tf.int64))) val_dataset = val_dataset.shuffle(1000).batch(batch_size) Simple MLP model Now we create a simple multi-layer perceptron model so that we can apply the ordinal output layer. def create_model(num_classes): model = tf.keras.Sequential() model.add(tf.keras.layers.Flatten(input_shape = (28, 28, ))) model.add(tf.keras.layers.Dense(128, activation = \"relu\")) model.add(tf.keras.layers.Dropout(0.2)) model.add(tf.keras.layers.Dense(32, activation = \"relu\")) model.add(tf.keras.layers.Dropout(0.1)) # No activation function specified so this will output cumulative logits. model.add(tf.keras.layers.Dense(num_classes-1)) return model model = create_model(NUM_CLASSES) # Note that the model generates 1 fewer outputs than the number of classes. model.summary() # Or a functional API version def create_model2(num_classes): inputs = tf.keras.Input(shape = (28, 28, )) x = tf.keras.layers.Flatten()(inputs) x = tf.keras.layers.Dense(128, activation = \"relu\")(x) x = tf.keras.layers.Dropout(0.2)(x) x = tf.keras.layers.Dense(32, activation = \"relu\")(x) x = tf.keras.layers.Dropout(0.1)(x) # No activation function specified so this will output cumulative logits. outputs = tf.keras.layers.Dense(num_classes-1)(x) model = tf.keras.Model(inputs = inputs, outputs = outputs) return model model = create_model2(NUM_CLASSES) # Note that the model generates 1 fewer outputs than the number of classes. model.summary() model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate), loss = condor.SparseCondorOrdinalCrossEntropy(), metrics = [condor.SparseOrdinalEarthMoversDistance(), condor.SparseOrdinalMeanAbsoluteError()]) history = model.fit(dataset, epochs = 5, validation_data = val_dataset, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)]) Test set evaluation # Evaluate on test dataset. model.evaluate(test_dataset) Cumulative logits to probabilities We can convert the cumulative logit output of the layer into the probability estimate for each ordinal label. This can then be used to calculate other metrics like accuracy or mean absolute error. Notice that the probability distribution for each observation is unimodal, which is what we want for an ordinal outcome variable. print(\"Predict on test dataset\") # Note that these are ordinal (cumulative) logits, not probabilities or regular logits. ordinal_logits = model.predict(test_dataset) # Convert from logits to label probabilities. This is initially a tensorflow tensor. tensor_probs = condor.ordinal_softmax(ordinal_logits) # Convert the tensor into a pandas dataframe. probs_df = pd.DataFrame(tensor_probs.numpy()) probs_df.head() # Check that probabilities all sum to 1 - looks good! probs_df.sum(axis = 1) Label prediction This notebook shows two ways of calculating predicted labels. We can take the highest probability label (first method) or we can choose the highest label with Pr(Y > label) > 50%. # Probs to labels labels = probs_df.idxmax(axis = 1) labels.values # What is our accuracy? Around 69%. np.mean(labels == mnist_labels_test) # Compare to logit-based cumulative probs cum_probs = pd.DataFrame(ordinal_logits).apply(special.expit).cumprod(axis=1) cum_probs.head() Now we should try another option, which is used in the Cao et al. paper. # Calculate the labels using the style of Cao et al. labels2 = cum_probs.apply(lambda x: x > 0.5).sum(axis = 1) labels2.head() # What is the accuracy of these labels? np.mean(labels2 == mnist_labels_test) # More often than not these are the same, but still a lot of discrepancy. np.mean(labels == labels2) print(\"Mean absolute label error version 1:\", np.mean(np.abs(labels - mnist_labels_test))) print(\"Mean absolute label error version 2:\", np.mean(np.abs(labels2 - mnist_labels_test))) mnist_labels_test[:5] Importance weights customization A quick example to show how the importance weights can be customized. model = create_model(num_classes = NUM_CLASSES) model.summary() # We have num_classes - 1 outputs (cumulative logits), so there are 9 elements # in the importance vector to customize. importance_weights = [1., 1., 0.5, 0.5, 0.5, 1., 1., 0.1, 0.1] loss_fn = condor.SparseCondorOrdinalCrossEntropy(importance_weights = importance_weights) model.compile(tf.keras.optimizers.Adam(lr = learning_rate), loss = loss_fn) history = model.fit(dataset, epochs = num_epochs)","title":"MNIST"},{"location":"tutorials/mnist/#condor-ordinal-classificationregression-in-tensorflow-keras","text":"","title":"CONDOR Ordinal classification/regression in Tensorflow Keras"},{"location":"tutorials/mnist/#import-statements","text":"import numpy as np import sklearn from sklearn import model_selection from sklearn.model_selection import train_test_split import pandas as pd from scipy import special import tensorflow_hub as hub import os import json import gzip from urllib.request import urlopen import tensorflow as tf print(\"Tensorflow version\", tf.__version__) import condor_tensorflow as condor print(\"CONDOR Ordinal version:\", condor.__version__)","title":"Import statements"},{"location":"tutorials/mnist/#mnist-toy-example","text":"This outcome is not actually ordinal, it's categorical. We're just using it as a toy example to show how the different components are used. # Hyperparameters random_seed = 1 # Not yet used learning_rate = 0.05 batch_size = 128 num_epochs = 2 # Architecture NUM_CLASSES = 10 # Fetch and format the mnist data (mnist_images, mnist_labels), (mnist_images_test, mnist_labels_test) = tf.keras.datasets.mnist.load_data() # Split off a validation dataset for early stopping mnist_images, mnist_images_val, mnist_labels, mnist_labels_val = \\ model_selection.train_test_split(mnist_images, mnist_labels, test_size = 5000, random_state = 1) print(\"Shape of training images:\", mnist_images.shape) print(\"Shape of training labels:\", mnist_labels.shape) print(\"Shape of test images:\", mnist_images_test.shape) print(\"Shape of test labels:\", mnist_labels_test.shape) print(\"Shape of validation images:\", mnist_images_val.shape) print(\"Shape of validation labels:\", mnist_labels_val.shape) # Also rescales to 0-1 range. dataset = tf.data.Dataset.from_tensor_slices( (tf.cast(mnist_images[..., tf.newaxis] / 255, tf.float32), tf.cast(mnist_labels, tf.int64))) dataset = dataset.shuffle(1000).batch(batch_size) test_dataset = tf.data.Dataset.from_tensor_slices( (tf.cast(mnist_images_test[..., tf.newaxis] / 255, tf.float32), tf.cast(mnist_labels_test, tf.int64))) #test_dataset = test_dataset.shuffle(1000).batch(batch_size) # Here we do not shuffle the test dataset. test_dataset = test_dataset.batch(batch_size) val_dataset = tf.data.Dataset.from_tensor_slices( (tf.cast(mnist_images_val[..., tf.newaxis] / 255, tf.float32), tf.cast(mnist_labels_val, tf.int64))) val_dataset = val_dataset.shuffle(1000).batch(batch_size)","title":"MNIST toy example"},{"location":"tutorials/mnist/#simple-mlp-model","text":"Now we create a simple multi-layer perceptron model so that we can apply the ordinal output layer. def create_model(num_classes): model = tf.keras.Sequential() model.add(tf.keras.layers.Flatten(input_shape = (28, 28, ))) model.add(tf.keras.layers.Dense(128, activation = \"relu\")) model.add(tf.keras.layers.Dropout(0.2)) model.add(tf.keras.layers.Dense(32, activation = \"relu\")) model.add(tf.keras.layers.Dropout(0.1)) # No activation function specified so this will output cumulative logits. model.add(tf.keras.layers.Dense(num_classes-1)) return model model = create_model(NUM_CLASSES) # Note that the model generates 1 fewer outputs than the number of classes. model.summary() # Or a functional API version def create_model2(num_classes): inputs = tf.keras.Input(shape = (28, 28, )) x = tf.keras.layers.Flatten()(inputs) x = tf.keras.layers.Dense(128, activation = \"relu\")(x) x = tf.keras.layers.Dropout(0.2)(x) x = tf.keras.layers.Dense(32, activation = \"relu\")(x) x = tf.keras.layers.Dropout(0.1)(x) # No activation function specified so this will output cumulative logits. outputs = tf.keras.layers.Dense(num_classes-1)(x) model = tf.keras.Model(inputs = inputs, outputs = outputs) return model model = create_model2(NUM_CLASSES) # Note that the model generates 1 fewer outputs than the number of classes. model.summary() model.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = learning_rate), loss = condor.SparseCondorOrdinalCrossEntropy(), metrics = [condor.SparseOrdinalEarthMoversDistance(), condor.SparseOrdinalMeanAbsoluteError()]) history = model.fit(dataset, epochs = 5, validation_data = val_dataset, callbacks = [tf.keras.callbacks.EarlyStopping(patience = 3, restore_best_weights = True)])","title":"Simple MLP model"},{"location":"tutorials/mnist/#test-set-evaluation","text":"# Evaluate on test dataset. model.evaluate(test_dataset)","title":"Test set evaluation"},{"location":"tutorials/mnist/#cumulative-logits-to-probabilities","text":"We can convert the cumulative logit output of the layer into the probability estimate for each ordinal label. This can then be used to calculate other metrics like accuracy or mean absolute error. Notice that the probability distribution for each observation is unimodal, which is what we want for an ordinal outcome variable. print(\"Predict on test dataset\") # Note that these are ordinal (cumulative) logits, not probabilities or regular logits. ordinal_logits = model.predict(test_dataset) # Convert from logits to label probabilities. This is initially a tensorflow tensor. tensor_probs = condor.ordinal_softmax(ordinal_logits) # Convert the tensor into a pandas dataframe. probs_df = pd.DataFrame(tensor_probs.numpy()) probs_df.head() # Check that probabilities all sum to 1 - looks good! probs_df.sum(axis = 1)","title":"Cumulative logits to probabilities"},{"location":"tutorials/mnist/#label-prediction","text":"This notebook shows two ways of calculating predicted labels. We can take the highest probability label (first method) or we can choose the highest label with Pr(Y > label) > 50%. # Probs to labels labels = probs_df.idxmax(axis = 1) labels.values # What is our accuracy? Around 69%. np.mean(labels == mnist_labels_test) # Compare to logit-based cumulative probs cum_probs = pd.DataFrame(ordinal_logits).apply(special.expit).cumprod(axis=1) cum_probs.head() Now we should try another option, which is used in the Cao et al. paper. # Calculate the labels using the style of Cao et al. labels2 = cum_probs.apply(lambda x: x > 0.5).sum(axis = 1) labels2.head() # What is the accuracy of these labels? np.mean(labels2 == mnist_labels_test) # More often than not these are the same, but still a lot of discrepancy. np.mean(labels == labels2) print(\"Mean absolute label error version 1:\", np.mean(np.abs(labels - mnist_labels_test))) print(\"Mean absolute label error version 2:\", np.mean(np.abs(labels2 - mnist_labels_test))) mnist_labels_test[:5]","title":"Label prediction"},{"location":"tutorials/mnist/#importance-weights-customization","text":"A quick example to show how the importance weights can be customized. model = create_model(num_classes = NUM_CLASSES) model.summary() # We have num_classes - 1 outputs (cumulative logits), so there are 9 elements # in the importance vector to customize. importance_weights = [1., 1., 0.5, 0.5, 0.5, 1., 1., 0.1, 0.1] loss_fn = condor.SparseCondorOrdinalCrossEntropy(importance_weights = importance_weights) model.compile(tf.keras.optimizers.Adam(lr = learning_rate), loss = loss_fn) history = model.fit(dataset, epochs = num_epochs)","title":"Importance weights customization"}]}